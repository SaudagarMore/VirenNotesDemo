APPLICATION: COLLECTION OF SERVICES
ARCHITECTURE : BLUE PRINT




====================================================

NGINX: 
IT IS A WEB SERVER
TI SHOWS APPLICATION

HOW TO INSTALL NGINX:

apt	: package manager -- > to install packages
install	: action  -- > what you are doing
nginx	: name of the package


STEP-1: apt install nginx
STEP-2: cd /var/www/html [in this path we put the front end code]
SETP-3: vim index.html   [writing the code on index.html]


=======================================================
OS: OPERATING SYSTEM
IT IS USED TO COMMUNICATE WITH SYSTEMS.

IS LINUX OS OR KERNEL ?
ANS: KERNEL

WHAT IS KERNEL ?
LOWEST LEVEL OF OS


COMMAND:

FILE COMMANDS:

mkdir virat	: to create a directory
cd virat	: to change directory
touch file1	: to create a file
ls/ll		: to list the files


===========================================================================


SERVER: SERVES THE SERVICES TO END USER.

=========================================================

LINUX COMMANDS: 

FILE COMMANDS:

touch file1	: used to create a file
ll		: to list the files
cat file1	: to show the content of a file
vim file1	: to insert the content in a file
i		: to write the content
esc :wq		: to save and exit from file

cp file1 file2	: to copy the content from file1 to file2
mv file1 file2	: to rename the file from file1 to file2
rm file1 -f	: to remove a file1
rm * -f		: to remove all the files

head file1	: to print top 10 lines of a file
head -15 file1	: to print top 15 lines of a file
head -5 file1	: to print top 5 lines of a file

tail file1	: to print bottom 10 lines of a file
tail -5 file1	: to print bottom 5 lines of a file
sed -n '12,19p' file1 : to print from line-12 to line-19
wc file1	: to find number of lines, words and characters

mkdir folder1	: to create a folder
cd folder1	: to go inside folder1
cd ..		: to come outside of folder
----------------------------------------------------------------
HARDWARE:
lscpu		: to show CPU information
lsmem		: to show RAM information
lsblk		: to show ROM information
lshw		: to show Complete Hardware information
free		: to show Available RAM information

======================================================
GREP: GLOBAL REGULAR EXPRESSION PRINT
PURPOSE: TO SEARCH FOR A WORD

grep word file1		: to search for a word in the file
grep WORD file1 -i	: -i -- > to avoid case sensitive
grep WORD file1 -i -c	: -c -- > to print the count
grep 'word1\|word2' file1: to search for multiple words in a file.


======================================================

SCRIPT:

.

LOAD BALANCER: USED TO DISTRIBUTE TRAFFIC TO SERVERS.
BY DEFUALT LOAD BALANCER USE ROUND ROBIN ALGORITHM.
 

TYPES:
1. APPLICATION LOAD BALANCER  [HTTP, HTTPS]
2. NETWORK LOAD BALANCER      [TCP, UDP]
3. CLASSIC LOAD BALANCER      [HTTP, HTTPS, TCP, UDP]
4. GATEWAY LOAD BALANCER  


NOTE: OSI = OPEN SOURCE INTERCONNECTION
APPLICATION
PRESENTATION
SESSION
TRANSPORT
NETWORK
DATA
PHYSICAL


TARGET : SINGLE SERVER
TARGET GROUP: GROUP OF SERVERS
LISTNER RULE: PORT ON WHICH APP WILL RUN

1. CLICK ON LOAD BANACER & CREATE
2. APPLICATION LOADBALANCER
Name: Swiggy
Scheme: Internet-facing (for web app)
Load balancer IP address type: IPV4
Network mapping: us-east-1a (use1-az2) & us-east-1b (use1-az4)
Listeners and routing:

Protocl: HTTP
Port: 80
Create Target Group:
Choose a target type: instances
Traget Group Name: swiggy-targetgroup
Next
Select Both Servers
Include aspending below
create target group
Go back to previous tab
click on refresh and select target group
create load balancer


AMI: AMAZON MACHINE IMAGE
CREAT A SERVER AND DEPLOY SWIGGY APP
SELECT SERVER -- > ACTIONS -- > IMAGES AND TEMPLATES -- > CREATE IMAGE -- > NAME -- > CREATE

==========================================================
ASG = AUTO SCALING GROUP
WHY: TO ADD/REMOVE SERVERS AUTOMATICALLY.
IF WE DELETE A SERVER ASG WILL RECREATE SAME SERVER.

LOAD IS HIGH -- > ADD THE NEW SERVERS
LOAD IS LOW  -- > TO DELETE EXISTING SERVERS

WHEN WE USE AUTO SCALING GROUPS:
IF THE LOAD IS CHANGING FREQUENTLY WE CAN USE ASG.

TYPES OF SCALING:
1. HORIZONTAL SCALING : WE CREATE NEW SERVERS 
2. VERTICAL SCALING   : FOR EXISTING SERVERS WE CAN INCREASE CPU & RAM

WEB & APP : HORIZONTAL SCALING
DB        : VERTICAL SCALING

TRACKING POLICY:
1. CPU 
2. NETWORK IN
3. NETWORK OUT
4. COUNT PER TARGET


TEMPLATE: IT CONSIST OF CONFIGURATION OF A SERVER WHICH IS CREATED BY ASG.
WITHOUT TEMPLATE WE CANT CREATE ALL SERVERS WITH SAME CONFIG.

STEPS TO CREATE ASG:
AUTO SCALING GROUP -- > CREATE
NAME: SWIGGY
CREATE A LAUNCH TEMPLATE
NAME: SWIGGY-TEMPLATE
NOTE: GIVE THE CONFIGURATION JUST LIKE WE GIVE FOR EC2.
CREATE TEMPLATE

AZ AND SUBNETS: US-EAST-1A, US-EAST-1B, US-EAST-1C
LOAD BALANCING: ATTACH TO A NEW LOAD BALANCER
SELECT CREATE A NEW TARGET GROUP
NEXT

DESIRED : 2 -- > HOW MANY SERVERS YOU WANT NOW
MIN: 2 -- > ALWAYS ATLEAST I WANT 2 SERVERS
MAX: 10 -- > IF LOAD IS INCREASE I WANT 10 SERVERS

AUTOMATIC SCALING POLICY:
TARGET TRACKING SCALING POLICY:
CPU -- > VALUE: 50 -- > NEXT

http://internal-swiggy-1-1906495425.ap-south-1.elb.amazonaws.com/

NOTIFICATION -- > CREATE A TOPIC -- > NAME: SWIGGY SERVERS -- > EMAIL: give your email -- > NEXT -- > NEXT -- > CREATE AUTO SCALING GROUP
 

HOW TO INCREASE LOAD:

LOGIN TO SERVER:
amazon-linux-extras install epel -y #(amazon Linux 2)
yum install stress -y #(amazon Linux 2023)
stress
stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 500s


NOTE: TO DELETE SERVERS WE NEED TO DELETE ASG.
=====================================================

MODIFIYING VOLUME:
SELECT SERVER -- > CLICK ON STORAGE -- > SELECT VOLUME -- > ACTIONS -- > MODIFY

ADDING ANOTHER VOLUME
CREATE A VOLUME
VOLUME -- > CREATE -- > SIZE: ABC -- > AZ: SELECT AS YOUR SERVER AZ -- > TAGS: Name = New server -- > CREATE

ATTACHING VOLUME TO SERVER:
SELECT VOLUME -- > ACTIONS -- > ATTACH -- > SELECT YOUR SERVER -- > /dev/xvdb -- > save

BOOT/ROOT VOL : WHICH HAS OS
NON-ROOT VOL: WHICH WILL NOT HAVE OS


SERVER MIGRATION:
1. CREATE A SERVER ON US-EAST-1A AND DEPLOY APP

2. TAKE A SNAPSHOT OF SERVER
SELECT SERVER -- > STORAGE -- > VOLUME -- > ACTIONS -- > CREATE SNAPSHOT -- > CREATE

3. CREATE EBS FROM SNAPSHOT
SELECT SNAPHSOT -- > ACTIONS -- > CREATE A VOLUME -- > AZ: US-EAST-2B -- > CREATE

4. CREATE A SERVER ON AP-SOUTH-1 & DETACH EXISTING VOLUME
CREATE A SERVER -- > SELECT -- > STOP 
VOLUME -- > SELECT -- > ACTIONS -- > DETACH

5. ATTACH THE VOLUME CREATED FROM SNAPSHOT
VOLUME -- > ACTIONS -- > ATTACH -- > SELECT SERVER -- > DEVICENAME: FIRST OPTION -- > ATTACH

COPYING THE SNAPSHOT:
SELECT SNAPSHOT -- > ACTIONS -- > COPY SNAPSHOT -- > DESTINATION: AP-SOUTH-1 -- > COPY

===============================================
IAM: IDENTITY & ACCESS MANAGEMENT
WHY TO USE: TO PROVIDE PERMISSIONS AND RESTRICTIONS TO USERS IN AWS.


AUTHENTICATION : PERMISSION TO LOGIN
AUTHORIZATION  : PERMISSION TO WORK


IN REAL TIME WE DONT USE ROOT USER FOR ROUTINE WORKS
WE USE AWS IAM USER IN REAL TIME.


IAM USER: 

IAM -- > USERS -- > CREATE USER -- > NAME: ABC -- > Provide user access to the AWS Management Console -- > I want to create an IAM user -- > Next -- > Attach policies directly -- > AmazonS3FullAccess -- > 


IAM GROUP:
USED TO GIVE PERMISSION FOR MULTIPLE USERS.
A SINGLE USER CAN BE PART OF MAX 10 GROUPS.
GROUP CANNOT BE NESTED.
IF USER HAVE PERMISSION ON GROUP LEVEL AND USER LEVEL BOTH OF THE APPLIES.

IAM -- > GROUP -- > NAME: DEVOPS -- > SELECT USERS -- > Attach policies directly -- > CREATE

IF U FROGOT USERNANE &PASSWORD:
SELECT USER -- > SECURITY CREDNTIALS -- > MANAGE CONSOLE ACCESS


IAM ROLES:
ROLES ARE USED BY SERVICES.
ROLE IS SIMLAR TO IAM USER.
ROLES ALSO HAVING PERMISSION LIKE USERS.
WE CAN SET TIME LIMIT FOR ROLES.
ROLE DOESNT HAVE ANY CREDS.
IF TWO SERVICES WANT TO COMMUNICATE OR WORK TOGETHER WE USE ROLES.
EX: EC2 -- > S3

http://54.242.59.171/

ROLE -- > CREATE ROLE -- > AWS SERVICE -- > EC2 -- > PERMISSIONS: S3 FULL ACCESS -- > NAME: S3 ROLE -- > CREATE

SELECT SERVER -- > ACTIONS -- > SECURITY -- > MODIFY IAM ROLE -- > S3 ROLE -- > UPDATE

DEPLOY THE APP FROM SCRIPT
command: aws s3 ls -- > execute this on server
aws s3 cp /var/log/httpd/access_log s3:bucketname

REVOKE SESSION: TO REMOVE THE EXISTING PERMISSION OF A ROLE.

INLINE POLICY: CREATING CUSTOM POLICY FOR RESOURCES

USER -- > ADD PERMISSION -- > CREATE INLINE POLICY -- > SERVICE: S3 -- > All S3 actions (s3:*) -- > bucket: BUCKETNAME -- > next

===========================================================
ACCESS KEY & SECRET ACCESS KEY = USED TO ASSIGN PERMISSION TO SERVER
BY DEFAULT WE CAN CREATE ONLY 2 KEYS.


USER -- > KEYS -- > ATTACH TO SERVER -- > SERVER

CREATE A USER -- > SECURITY CREDENTIALS -- > CREATE ACCESS KEY -- > 
Command Line Interface (CLI)-- > create 


aws configure -- > Run this command on server 

AWS Access Key ID [None]: ***********************
AWS Secret Access Key [None]: ************************
Default region name [None]: ap-south-1
Default output format [None]: table


CLI -- > COMMAND LINE INTERFACE
WHY TO USE: TO OPERATE/CONTROL AWS SERVICES THROUGH COMMANDS

aws s3 ls 			: to list the buckets
aws s3 ls s3://bucketname	: to list files inside the bucket
aws s3 mb s3://newbucket	: to create a bucket
aws s3 rb s3://newbucket	: to delete a bucket
aws s3 cp file s3://bucket	: to copy file1 to bucket
aws s3 cp folder s3://bucket --recursive : to copy folder to bucket
aws s3 cp copy-s3-uri .		: to copy files from bucket to server
aws s3 sync s3://bucket1 s3://bucket2	: to copy all files from one bucket to another.


IAM:
aws iam list-users		: TO LIST IAM USERS
aws iam list-groups		: TO LIST IAM GROUPS
aws iam list-roles		: TO LIST IAM ROLES

aws iam create-group --group-name devops
aws iam create-user --user-name viratabc
aws iam create-role --role-name demorole

aws iam delete-group --group-name devops
aws iam delete-user --user-name viratabc
aws iam delete-role --role-name demorole

EC2:

aws ec2 run-instances --image-id ami-04a37924ffe27da53 --instance-type t2.micro --count 
aws ec2 describe-instances   : to show complete info of all the servers
aws ec2 stop-instances --instance-ids i-054a200a4effc917c
aws ec2 start-instances --instance-ids i-054a200a4effc917c
aws ec2 terminate-instances --instance-ids i-054a200a4effc917c


mysql -u admin -p  

===============================================================

CIDR: CLASSLESS INTER DOMAIN ROUTING

VPC   =   WALL

STEP-1: CREATE VPC
VPC -- > CREATE VPC -- > NAME: SWIGGY -- > CIDR: 10.0.0.0/16 -- > CREATE

SUBNET = ROOM

STEP-2: CREATE SUBNET
SUBNET -- > CREATE -- > VPC: SWIGGY -- > NAME: WEB-SUBNET  -- > CIDR: 10.0.0.0/24 -- > CREATE

STEP-3: CREATE A INTERNET GATEWAY
INTERNET GATEWAY -- > CREATE -- > NAME: SWIGGY-IGW -- > CREATE -- > ATTACH TO VPC -- > SWIGGY

STEP-4: CREATE ROUTE TABLE
ROUTE TABLE -- > NAME: SWIGGY-WEB-RTB -- > VPC: SWIGGY -- > CREATE
EDI ROUTES -- > ADD -- > 0.0.0.0/0 -- > IGW : SWIGGY -- > ADD

ASSOCIATE THE WEB SERVER TO THE ROUTE TABLE.
SUBNET ASSOCIATION -- > SELECT WEBSUNET -- > ASSOCIATE.

CREATE  A WEB SERVER WITH SWIGGY VPC AND WEB SUBNET.
NOTE: ENABLE AUTO ASSIGN PUBLIC IP FOR WEB SERVER WHILE CREATING.


STEP-5: CREATE SUBNET
SUBNET -- > CREATE -- > VPC: SWIGGY -- > NAME: APP-SUBNET  -- > CIDR: 10.0.1.0/24 -- > CREATE

STEP-6: CREATE A NAT GATEWAY
NAT GATEWAY -- > CREATE -- > NAME: SWIGGY-NAT -- > SUBNET: WEB SUBNET -- > Elastic IP allocation -- > CREATE

STEP-7: CREATE ROUTE TABLE
ROUTE TABLE -- > NAME: APP-RTB -- > VPC: SWIGGY -- > CREATE
EDI ROUTES -- > ADD -- > 0.0.0.0/0 -- > IGW : SWIGGY -- > ADD
ASSOCIATE THE APP SERVER TO THE ROUTE TABLE.


NOW CONNECTING TO APP SERVER:

OPEN WEB SERVER & sudo -i
vim pemfile -- > copy & paste the pem file content -- > chmod 400 pemfile
ssh "pemfile" ec2-user@public-ip 

PEERING: ESTABLISHING CONNECTION BLW 2 VPCS

==========================================================================

INFRASTRUCTURE:
resources used to run our application on cloud.
ex: ec2, s3, elb, vpc, Asg --------------


in general we used to deploy infra on manual 

Manual:
1. time consume
2. Manual work
3. committing mistakes

Automate -- > Terraform -- > code -- > hcl (Hashicorp configuration languge)


WHAT IS TERRAFORM:
its a tool used to make infrastructure automation.
its a free and not open source.
its platform independent.
it comes on the year 2014.
who: Mitchel Hashimoto 
owned: Hashicorp -- > recently IBM is maintaining.
terraform is written on the go language.
We can call terraform as IAC TOOL.
it is Cloud Agonistic (it can used to work with any cloud and even on prem)
it can manage things from onprem also.


HOW IT WORKS:
terraform uses code to automate the infra.
we use HCL : HashiCorp Configuration Language.

IAC: Infrastructure as a code.

Code --- > execute --- > Infra 

ADVANTAGES:
1. Reusable 
2. Time saving
3. Automation
4. Avoiding mistakes
5. Dry run (Dont Repeat Yourself)

CLOUD ALTERNATIVES:
CFT = AWS
ARM = AZURE
GDE = GOOGLE

TERRAFROM = ALL CLOUDS

SOME OTHER ALTERNATIVES:
PULUMI
OpenTofu
ANSIBLE
CHEF
PUPPET



TERRAFORM VS ANSIBLE:
Terraform will create server
and these servers will be configure by ansible.


INSTALLING TERRAFORM:

sudo yum install -y yum-utils shadow-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install terraform

aws configure (or) give a role
check ll .aws/ for the configuration 

MAIN ITEMS IN FILE:
blocks
lables (name, type of resource)
arguments


Configuration files:
it will have resource configuration.
here we write inputs for our resource 
based on that input terraform will create the real world resources on your cloud.
extension is .tf 

mkdir terraform
cd terraform

vim main.tf 

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
ami = "ami-03eb6185d756497f8"
instance_type = "t2.micro"
}

I : INIT
P : PLAN
A : APPLY
D : DESTROY


TERRAFORM FMT :The terraform fmt command formats Terraform configuration file contents so that it matches the canonical format and style. This command applies a subset of the Terraform language style conventions, along with other minor adjustments for readability.


TERRAFORM COMMANDS:
terraform init	: initialize the provider plugins on backend
it will store information of plugins in .terraform folder
without plugins we cant create resources.
each provider will have its own plugins.
we can get every provider from terraform registry.
once plugins are downloaded we should not need to run init every time.


terraform plan	: to create an execution plan
it will take inputs given by users and plan the resource creation
if we haven't given inputs for few fields it will take default values.

terraform apply : to create resources
as per the given inputs on configuration file it will create the resources in real word.

terrafrom destroy : to delete resources
provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = 5
ami = "ami-03eb6185d756497f8"
instance_type = "t2.micro"
}


terraform apply --auto-approve
terraform destroy --auto-approve


STATE FILE: used to store the resource information which is created by terraform
to track the resource activities
in real time entire resource info is on state file.
we need to keep it safe & Secure
if we lost this file we cant track the infra.
Command:
terraform state list

terraform target: used to destroy the specific resource 
terraform state list
single target: terraform destroy -auto-approve -target="aws_instance.one[3]"
multi targets: terraform destroy -auto-approve -target="aws_instance.one[1]" -target="aws_instance.one[2]"


=================================================================================

TERRAFORM VARIABLES:
when the values will change we will use variables.
in real time we keep all the variables in variable.tf to maintain the variables easily.



provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0b41f7055516b991a"
instance_type = var.instance_type
}

variable "instance_type" {
default = "t2.micro"
}

variable "instance_count" {
description = "*"
type = number
default = 5
}

terraform apply --auto-approve
terraform destroy --auto-approve

TERRAFORM FMT: 
used to give allignment and indentation for terraform files.
it rewrite configuration files to a canonical format and style.

Terraform tfvars:
When we have multiple configurations for terraform to create resource
we use tfvars to store different configurations.
on execution time pass the tfvars to the command it will apply the values of that file.

NOTE: BY DEFAULT TERRAFORM WILL PICK VALUES FROM TERRAFORM.TFVARS 
if you don't have terraform.tfvars we need to specify the tfvar file while executing.

ALTERNATE NAME:
terraform.tfvars.json

cat main.tf.   
provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "one" {
count = var.instance_count
ami = "ami-0e001c9271cf7f3b9"
instance_type = var.instance_type
tags = {
Name = var.instance_name
}
}

cat variable.tf
variable "instance_count" {
}

variable "instance_type" {
}

variable "instance_name" {
}

cat dev.tfvars
instance_count = 1

instance_type = "t2.micro"

instance_name = "dev-server"

cat test.tfvars
instance_count = 2

instance_type = "t2.medium"

instance_name = "test-server"

cat prod.tfvars
instance_count = 3

instance_type = "t2.large"

instance_name = "prod-server"

terraform apply -auto-approve -var-file="dev.tfvars"
terraform apply -auto-approve -var-file="test.tfvars"
terraform apply -auto-approve -var-file="prod.tfvars"

rm -rf *tfvars

TERRAFORM CLI: 

cat main.tf
provider "aws" {
}

resource "aws_instance" "one" {
count = var.var.instance_name
ami = "ami-00b8917ae86a424c9"
instance_type = var.instance_type
tags = {
Name = var.instance_name
}
}

cat variable.tf
variable "instance_count" {
}

variable "instance_type" {
}
 

variable "instance_name" {
}


METHOD-1:
terraform apply --auto-approve
terraform destroy --auto-approve

METHOD-2:
terraform apply --auto-approve -var="instance_type=t2.micro" 
terraform destroy --auto-approve -var="instance_type=t2.micro"

NOTE: If you want to pass single variable from cli you can use -var or if you want to pass multiple variables from cli create terraform .tfvars files and use -var-file.


TERRAFORM OUTPUTS:
Whenever we create a resource by Terraform if you want to print any output of that resource we can use the output block this block will print the specific output as per our requirement.


provider "aws" {
}

resource "aws_instance" "one" {
ami = "ami-00b8917ae86a424c9"
instance_type = "t2.micro"
tags = {
Name = "nani-server"
}
}


output "one" {
value = [aws_instance.one.public_ip, aws_instance.one.private_ip, aws_instance.one.public_dns]0
}

TO GET COMPLTE OUTPUS:

output "one" {
value = aws_instance.one
}


Note: when we change output block terraform will execute only that block
remianing blocks will not executed because there are no changes in those blocks.

===================================================================

TERRAFORM TAINT
It allows for you to manually mark a resource for recreation
in real time some times resources fails to create so to recreate them we use taint
in new version we use -replace option
TO TAINT: terraform taint aws_instance.one[0]
TO UNTAINT: terraform untaint aws_instance.one[0]
terraform state list
terraform taint aws_instance.one[0]
terraform apply --auto-approve 

TERRAFORM REPLACE:
terraform apply --auto-approve -replace="aws_instance.one[0]"


CODE:
statefilebucketfroterraform

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  count         = 2
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "virat"
  }
}


TERRAFORM IMPORT
when we create resource manually terraform wont track that resource.
Import command can used to import the resource which is created manually.
can only import one resource at a time (FROM CLI)
it can import both code to config file and state file.
FOR STATE FILE: terraform import aws_instance.one (not prefarable)

FIRST CREATE A MANUAL SERVER.

cat main.tf

provider "aws" {
  region = "us-east-1"
}

import {
  to = aws_instance.one
  id = "i-0c23bdc1b7b73d61c"
}

FOR STATEFILE & CODE: 
terraform plan -generate-config-out=ec2.tf
NOTE: DELETE LINE 16 AND 17 FROM ec2.tf
terraform apply -auto-approve

WORKSPACES:
Terraform workspaces allow you to maintain multiple environments 
       (e.g., development, testing, production) 
using the same Terraform configuration but with different states. 
Each workspace has its own state file (terraform.tfstate.d folder)
 resources created/managed in one workspace do not affect other workspaces.
the default workspace in Terraform  is default 
Each workspace is isloated (seperated with each other)


terraform workspace list : to list the workspaces
terraform workspace new dev : to create workspace
terraform workspace show : to show current workspace
terraform workspace select dev : to switch to dev workspace
terraform workspace delete dev : to delete dev workspace

    NOTE:
       1. we need to empty the workspace before delete
       2. we cant delete current workspace, we can switch and delete
       3. we cant delete default workspace

EXECUTION:
NOTE: TAKE CODES FROM TFVARS CONCEPT.

terraform workspace new dev
terraform apply -auto-approve -var-file="dev.tfvars"

terraform workspace new test
terraform apply -auto-approve -var-file="test.tfvars"

terraform workspace new prod
terraform apply -auto-approve -var-file="prod.tfvars"


FOR DELETION:

terraform destroy -auto-approve -var-file="prod.tfvars"
terraform workspace select test
terraform workspace delete prod

terraform destroy -auto-approve -var-file="test.tfvars"
terraform workspace select dev
terraform workspace delete test

terraform destroy -auto-approve -var-file="dev.tfvars"
terraform workspace select default
terraform workspace delete dev



TERRAFORM STATE COMMANDS

The terraform state command is used for advanced state management.
There are some cases where you may need to modify the Terraform state.
Rather than modify the state directly use these commands.

terraform state list : to list the resources
terraform state show aws_subnet.two : to show specific resource info
terraform state mv aws_subnet.two aws_subnet.three : to rename block
terraform state rm aws_subnet.three : to remove state information of a resource
terraform state pull : to pull state file info from backend


TERRAFORM DEBUGGING
Terraform automates the infrastructure.
issues like misconfigurations or dependency errors can occur. 
Debugging helps understand issues and fix them efficiently.
You can set TF_LOG to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs, with TRACE being the most verbose.
export TF_LOG=TRACE 
export TF_LOG_PATH="logs.txt"
terraform apply

==============================================================

1. what is default backend: local
2. does local backend support state locking: no
3. why to lock state file: to prevent from corruption
4. when state file will lock: when two people work on same state file parallely
5. why use s3 & dynamodb: to lock state file automatically.


TERRAFORM STATE FILE
Terraform Stores the infrastructure infromation on state file.
it will automatically refreshed when we run plan, apply & destroy.
In Terraform, a backend is a configuration that determines how and where Terraform stores its state file and how it manages operations like apply, plan, and destroy.
 By default Terraform uses local backend. 
it stores state file in terraform.tfstate in local folder.
it stores information in json format.
if we delete any resource it stores infromation in terraform.tfstate.backup.


TERRAFORM STATE FILE LOCKING
in Real time once we complete our work we need to lock state file.
it ensures that only one operation can be executed at a time.
once you lock state file you cant modify the infrastructure anymore.
When two people working on state file at a time it will be locked automatically.
unfortunately if two people runs apply at same time unpredictable results, like creating duplicate resources or destroying the wrong infrastructure.
Not all Terraform backends support locking.


Terraform used local backend to manage state file.
But in that local backend only one person can able to access it.
in Real time it’s often necessary to have a centralized, consistent, and secure storage mechanism for the state file. 
Amazon S3 is a popular choice for this, and when combined with DynamoDB for locking, it ensures safe, consistent operations.


TERRAFORM S3 BACKEND

ADVANTAGES
Global Access
Team Collaboration
Secure and Scalable
Centralized State Management
State File Versioning
Disaster Recovery

WHY LOCKING HAPPEND

when 2 developers  work on the same project with same state file then  the locking will be happend.
if state file is locked only first operation execute and second operation waits.
to remove state lock use: terraform force-unlock <LOCK_ID>
after adding dynamodb run:  terraform init -reconfigure

Add that block to existing code and run terraform init -upgrade
dynamodb -- > create table -- > Partition key: LockID -- > create
now after apply state file will go to s3 bucket
dev-1 type destroy and dev-2 type apply now state file locked.
you can check lock-id in new items of table.
once destroy done for dev-1 state file will be unlocked and dev-2 can work.

CODE:

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "s3" {
    bucket = "mybucket"
    key    = "path/to/my/key"
    region = "us-east-1"
    dynamodb_table = "tablename"
  }
}

resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}



MIGRATING FROM  S3 TO LOCAL BACKEND

if we want state file to back on local use below method.
remove backend code from main.tf 
run terraform init -migrate-state

CODE:

provider "aws" {
  region = "us-east-1"
}


resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}


TERRAFORM REFRESH

This command will be use to refresh the state file.
terraform compares the current state to desired state,  
       on the current state it will update values to state file.
when we run plan, apply or destroy refresh will perform automatically.
if a server is manually created running terraform apply -refresh-only would detect those changes and update the state file to reflect the current state of the resource, but it won't attempt to change the infrastructure to match the Terraform configuration.
if you dont want to refresh while apply & destroy use 
        terraform apply/destroy -refresh=false


TERRAFORM BACKEND BLOCK

By default there is no backend configuration block within Terraform configuration Because Terraform will use it's default backend - local 
This is why we see the terraform.tfstate file in our working directory. 
FOR PARTIAL BACKEND: 
        path = "state_data/terraform.dev.tfstate"
FROM CLI: 
        terraform init -backend-config="path=state_data/terraform.prod.tfstate" -migrate-state
If want we can specify multiple partial backends too.

CODE:

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "local" {
    path = "/tmp/abc.tfstate"
  }
}


resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}

TERRAFORM SENSITIVE DATA

By default the local state is stored in plain text as JSON. 
There is no additional encryption beyond your hard disk. 
Terraform can store sensitive information in plain text.
Amazon S3 & Terraform cloud for  you can enable encryption

variable "first_name" {
  type = string
   sensitive = true
   default = "Terraform"
}
check state file it will show data.



Treat State as Sensitive Data
Encrypt State Backend
Control Access to State File
BEST PRACTICE

CODE:

provider "aws" {
  region = "us-east-1"
}


resource "aws_instance" "one" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = var.instance_name
  }
}

variable "instance_name" {
default = "root"
}

output "abc" {
sensitive = true
value = aws_instance.one.tags_all
}
===========================================================

TERRAFORM VALIDATE:{20}
used validates the configuration files in your working directory.
it will show error when we havent given the values for variables.
command: terraform validate


TERRAFORM PLAN:

used to save plan in a file for future reference.
command: terraform plan -out myplan
  to apply    : terraform apply myplan
  to destroy: terraform plan -destroy

PROVIDER BLOCK:

By default provider plugins in terraform change version for every few weeks.
when we run init command, it download latest plugins always.
some code will not work with old plugins, so we need to update them.
To get latest provider plugins : https://registry.terraform.io/browse/providers.
when you add a new provider terraform init is must.
terraform providers: to list the providers which required to run code.
to create infra on any cloud all we need to have is provider.

TYPES:
1. OFFICIAL : MANAGED BY TERRAFORM
2. PARTNER  : MANAGE BY 3RD PATRY COMPANY
3. COMMUNITY: MANAGED BY INDIVIDUALS


CODE:

provider "aws" {
  region = "us-east-1"
}


terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = ">5.70.0"
    }
  }
}


terraform init -upgrade


MULTI PROVIDERS:

terraform {
  required_version = ">=1.9.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = ">5.70.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "4.9.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "6.10.0"
    }
  }
}

cat .terraform.lock.hcl
terraform provider


TERRAFORM BLOCK:

This block is used to set global configurations and settings for the Terraform.
It usually includes details such as required providers, backend configuration, and version constraints.
terraform -v
terraform -version
terraform --version

TERRAFORM LOCAL BLOCK:

A local block is used to define  values.
if a value is repeating multiple times we can define it here.
This makes our code cleaner and easier to understand.
simply define value once and use for mutiple times.

CODE:

provider "aws" {
  region = "us-east-1"
}

locals {                      env = "test"
}

resource "aws_vpc" "one" {
  cidr_block = "10.0.0.0/16"       
  tags = {
    Name = "${local.env}-vpc"
  }
}

resource "aws_subnet" "two" {
  vpc_id     = aws_vpc.one.id
  cidr_block = "10.0.1.0/24"

  tags = {
    Name = "${local.env}-subent"
  }
}

resource "aws_instance" "three" {                            #  //  /*  */
  subnet_id     = aws_subnet.two.id
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "${local.env}--server"
  }
}


TERRAFORM COMMENTS:
We use comments to make others code to understand easily.
Terraform supports three different syntaxes for comments.
#   -- > single line comment
//  -- > single line comment
/*   */  -- > multi line comment
Note: if we put comments for code, terraform thinks code is not existed and it will destroy the resource.

TLS PROVIDER:

it provides utilities for working with Transport Layer Security keys & certificates. 
It provides resources that allow private keys, certificates & CSR.
Add tls on your own and try this below code.

CODE:

provider "aws" {
  region = "us-east-1"
}

resource "tls_private_key" "rsa-4096-example" {
  algorithm = "RSA"
  rsa_bits  = 4096
}


resource "local_file" "private_key_pem" {
  content  = tls_private_key.rsa-4096-example.private_key_pem
  filename = "devops.pem"
}

LOCAL EXEC:
executes  command/script on local machine (where terraform is installed)
it will execute the command when resource is created

CODE:

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "three" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "abc-server"
  }
  provisioner "local-exec" {
    command = "echo this is my local server"
  }
}


REMOTE EXEC:

executes  command/script on remote machine.
once the server got created it will execute the commands and scripts for 
installing the softwares and configuring them and deploying app also.


CODE:

provider "aws" {
}

resource "aws_instance" "one" {
  ami                    = "ami-04823729c75214919"
  instance_type          = "t2.micro"
  key_name               = "rahull"
  vpc_security_group_ids = ["sg-05f044979e305302e"]
  tags = {
    Name = " viratinstance"
  }

  provisioner "remote-exec" {
    inline = [
      "sudo yum install httpd git -y",
      "sudo systemctl start httpd",
      "sudo cd /var/www/html",
      "sudo git clone https://github.com/karishma1521success/swiggy-clone.git",
      "sudo mv swiggy-clone/* .",
      "sudo mv /home/ec2-user/* /var/www/html"
    ]

    connection {
      type        = "ssh"
      user        = "ec2-user"
      private_key = file("~/.ssh/id_rsa")
      host        = self.public_ip
    }
  }
}

ALIAS & PROVIDERS:
we can map resource blocks to particular provider blocks.
used to create resources on different regions.

CODE:

provider "aws" {
region = "us-east-1"
}

resource "aws_instance" "three" {
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server"
  }
}


provider "aws" {
region = "ap-south-1"
alias = "south"
}

resource "aws_instance" "abcd" {
  provider      = aws.south
  ami           = "ami-08bf489a05e916bbd"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server"
  }
}

=========================================================================

MODULES:
it divides the code into folder structure.
Modules are group of multiple resources that are used together.
This makes your code easier to read and reusable across your organization.
we can publish modules for others to use.
each module will be having sperate plugins.
modules plugins will be store on .terraform/modules/

TYPES:
Root Module: This is the main  directory where Terraform commands are run. 
All Terraform configurations belong to the root module.

Child Modules: These modules are called by other modules.

.
├── main.tf
├── modules
│   ├── my_instance
│   │   └── main.tf
│   ├── s3_module
│   │   └── main.tf
│   └── vpc_module
│       └── main.tf

CODE:

cat main.tf

provider "aws" {
  region = "us-east-1"
}


module "vpc" {
  source = "./modules/vpc_module"
}

module "ec2" {
  source = "./modules/my_instance"
}

module "s3" {
  source = "./modules/s3_module"
}


cat modules
modules/my_instance/main.tf

resource "aws_instance" "one" {
  ami           = "ami-0ddc798b3f1a5117e"
  instance_type = "t2.micro"
  tags = {
    Name = "module-server"
  }
}


cat modules/s3_module/main.tf
resource "aws_s3_bucket" "example" {
  bucket = "viratdemo-tf-test-bucket"
}

cat modules/vpc_module/main.tf
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "module-vpc"
  }
}




MODULE INPUT & OUTPUTS:T
We can add Input and Output Blocks for Terraform Modules 
Root module can refer both variables & values of child modules.
Child modules cant refer variables, but it can refer its values.

.TERRAFORM/MODULES

├── main.tf
├── modules
│   └── myec2
│       ├── main.tf
│       ├── output.tf
│       └── variable.tf


cat main.tf
provider "aws" {
region = "us-east-1"
}

module "server" {
source = "./modules/myec2"
}

output "module_output" {
value = module.server.public_ip
}


cat modules/myec2/main.tf
resource "aws_instance" "one" {
  ami           = var.ami
  instance_type = var.instance_type
  tags = {
    Name = var.instance_name
  }
}

cat modules/myec2/variable.tf
variable "ami" {
  default = "ami-0ddc798b3f1a5117e"
}

variable "instance_type" {
  default = "t2.micro"
}

variable "instance_name" {
  default = "module-server"
}

cat modules/myec2/output.tf
output "public_ip" {
  value = aws_instance.one.public_ip
}


PUBLIC MODULES:

The module must be on GitHub and must be a public repo.
NAMING FORMAT: terraform-<provider>-<name> (terraform-aws-ec2-instance)
must have a description.
 module structure will be main.tf, variables.tf, outputs.tf.
x.y.z tags for releases.


TERRAFORM GRAPH:
it shows the relationships between objects in a Terraform configuration.
using the DOT language.
once create infra run the command: terraform graph
Go to Google -- > Graphwiz online & copy paste the code

CODE:

provider "aws" {
region = "us-east-1"
}

resource "aws_vpc" "one" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "dev-vpc"
  }
}

resource "aws_subnet" "two" {
  vpc_id     = aws_vpc.one.id
  cidr_block = "10.0.1.0/24"

  tags = {
    Name = "dev-subent"
  }
}

resource "aws_instance" "three" {
  subnet_id     = aws_subnet.two.id
  ami           = "ami-0866a3c8686eaeeba"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server"
  }
}

NAME: Graphwiz online
========================================================================================



HCP CLOUD:

HCP means HashiCorp Cloud Platform  
it is a managed platform to automate cloud infrastructure.
it provide privacy, security and isloation.
it supports multiple providers like AWS, Azure, and Google Cloud. 
it offers a suite of open-source tools for managing infrastructure, including Terraform, Vault, Consul, and Nomad. 
We can use Different Code Repos for a Project.
We can use Variable sets to apply same variables to all the workspaces.

ACCOUNT CREATION:

Go to google & type : HCP CLOUD ACCOUNT SIGNIN
EMAIL & PASSWORD
VERIFY THE EMAIL -- > CREATE ORG
CLICK ON TERRAFORM
CONTINUE WITH HCP ACCOUNT
CREATE A GITHUB ACCOUNT
create repo -- > name -- > create -- > add new file -- > write terraform code -- >  commit

WORKING WITH HCP:

CREATE AN ORGINIZATION
CREATE YOUR WORKSPACE
INTEGRATE YOUR VCS  -- > GITHUB -- > SELECET REPO -- > NEXT -- > CONTINUE
ADD VARAIBLES -- > 
AWS_ACCESS_KEY_ID : MARK AS ENV VARS -- >  SENSITIVE -- > SAVE 
AWS_SECRET_ACCESS_KEY: MARK AS ENV VARS -- > SENSITIVE -- > SAVE 
NOTE: MARK THEM AS ENV VARIBALE AND MAKE SURE NO SPACES ARE GIVEN

RUNS -- > NEW RUN -- > START  -- > confirm and apply
IT WILL AUTOMATICALLY PLAN & WE NEED TO APPLY BY MANUAL
SECOND TIME WHEN WE CHANGE CODE IT WILL AUTOMATICALLY PLAN  
PLAN & APPLY
DESTROY


Cost Estimation policy: we can estimate the cost of infra before we create it.
by default this feature is disable, we need to enable 
click on terraform logo --  > orginization -- > settings -- > Cost Estimation -- > enable


SENTINEL POLICY: ITS A POLICY AS A CODE.
BY THIS SENTINEL POLICY WE CAN WRITE OUR OWN CONDITIONS.
IT WILL CHECK THE CONDITION OF RESOURCE BEFORE IT CREATED.
IF CONDITION IS SATISFIED IT WILL CREATE RESOURE, OTHERWISE IT WONT.
EX: TAGS, VERIFIED AMIS, SG --

TERRAFORM LOGO -- > ORG -- > SETTINGS -- > POLICY 



TERRAFORM CLOUD FEATURES:
1. Workspaces
2. Projects
3. Runs
4. Variables and Variable Sets
5. Policies and Policy Sets
6. Run Tasks
7. Single Sign-On (SSO)
8. Remote State
9. Private Registry
10. Agents
11. Role-Based Access Control
12. Version Control Integration
13. Observability

TERRAFORM CLOUD ENTERPISE FEATURES:
Private Module Registry: Includes a private registry for sharing modules securely across teams.
Policy as Code: Integrates Sentinel for enforcing policies during provisioning.
Enhanced Automation: Offers advanced run triggers, notifications, and support for custom workflows.
Multi-Organization Support: Allows multiple teams or departments to manage their own infrastructure within a single account.
Advanced Collaboration: Provides role-based access controls and team management features, allowing for fine-grained permissions.
Enhanced Security: Features like SSO (Single Sign-On), audit logs, and compliance tools.

Secure Secrets in Terraform Code:
TIP 1: Do Not Store Secrets in Plain Text
TIP 2: Mark Variables as Sensitive
TIP 3: Environment Variables
TIP 4: Secret Stores (e.g., Vault, AWS Secrets manager)

NOTE: Even after marked a value as sensitive in tf file, they are stored within the Terraform state file. 
It is recommended to store security creds outside of terraform.



Variable precedence :


export TF.var.instance_type=""

Build in function:


terraform console

> max(40,50,60)
60

> min(40,50,60)
40

> lower("HEllo")
hello


List vs Set

list = collection of value , it allows duplication values and also it follows indexing.(0,1,2,3,4.......N)


provider "aws" {
region = "ap-south-1" 
}

resource "aws_instance" "one" {
count = 3
ami = "ami-3740s749hf99"
instance_type = toset(var.instance_type) 
tags {
Name = tolist(var.instance_name) [count.index]
	}
}

variable "instance_type" {
type = set(string)
default = ["t2.micro","t2.medium", "t2.large"]
}

variable "instance_name" {
default = ["dev-server","test-server", "prod-server"]
}

===================================================================
META ARGUMENTS:

PARALLELISM: by default terraform follows parallelism.
it will execute all the resources at a time.
by default parallelism limit is 10.
terraform apply -auto-approve -parallelism=1
NOTE: IT WILL APPLICALBLE FOR BOTH APPLY & DESTROY.


DEPENDS_ON: one resource creation will be depending on other.
this is called explicit dependency.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "virat-server"
  }
depends_on = [aws_s3_bucket.two]
}

resource "aws_s3_bucket" "two" {
  bucket     = "terraformbucketabcd123"
}

2. COUNT: used to create similar objects.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  count         = 3
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "dev-server-${count.index+1}"
  }
}

NOTE: it wont create resources with different configs.

3. FOR_EACH: it is a loop used to create resources.
we can pass different configuration to same code.
it will create resource with less code.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  for_each      = toset(["dev-server", "test-server", "prod-server"])
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "${each.key}"
  }
}



4. LIFECYCLE: 

PREVENT DESTROY: used to prevent the resource to not be deleted.
it is bool.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "virat-server"
  }
  lifecycle {
    prevent_destroy = true
  }
}

NOTE: CHECK WITH TERRAFORM APPLY

IGNORE CHANGES:
when user modify anything on current state manually changes will be ignored.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-046d18c147c36bef1"
  instance_type = "t2.micro"
  tags = {
    Name = "virat-server"
  }
  lifecycle {
    ignore_changes = all
  }
}

NOTE: MODIFY NAME MANUALLY AND CHECK WITH TERRAFORM APPLY

CREATE BEFORE DESTROY:
by default when we modify some properties terraform will first destroy old server and create new server later.

if we follow create before destroy lifecycle the new resource will create first
and later old resource is going to be destroyed.

provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "virat-server"
  }
  lifecycle {
    create_before_destroy = true
  }
}


NOTE: CHNAGE AMI ID AND GIVE APPLY

SET: to eliminate the duplicate values

provider "aws" {
region = "us-east-1"
}


resource "aws_instance" "example" {
  for_each      = var.instance_type
  ami           = "ami-063d43db0594b521b"
  instance_type = each.value
}

variable "instance_type" {
  type        = set(string)
  default     = ["t2.micro", "t2.medium", "t2.large"]
}

DYNAMIC BLOCK:
it is used to reduce the length of code and used for reusabilty of code in loop.

provider "aws" {
  region = "us-east-1"
}

locals {
  ingress_rules = [{port = 443}, {port = 80}, {port = 22}, {port = 8080}]
}


resource "aws_instance" "ec2_example" {
  ami                    = "ami-0c02fb55956c7d316"
  instance_type          = "t2.micro"
  vpc_security_group_ids = [aws_security_group.main.id]
  tags = {
    Name = "Terraform EC2"
  }
}


resource "aws_security_group" "main" {
  egress = [
    {
      cidr_blocks      = ["0.0.0.0/0"]
      description      = "*"
      from_port        = 0
      ipv6_cidr_blocks = []
      prefix_list_ids  = []
      protocol         = "-1"
      security_groups  = []
      self             = false
      to_port          = 0
  }]

  dynamic "ingress" {
    for_each = local.ingress_rules

    content {
      description = "*"
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  }

  tags = {
    Name = "terra sg"
  }
}


STATE FILE VIEW FROM HCP TO LOCAL:
REMOTE BACKEND FROM HPC:
we can manage the logs/runs of terraform from HCP to Local.

terraform login
yes
copy the link and paste on browser.
generate token and give to terminal
add backend code and give commands 
see the output from HCP CLOUD.

provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "remote" {
    hostname = "app.terraform.io"
    organization = "swiggy0099"

    workspaces {
      name = "terraform"
    }
  }
}

resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "virat-server"
  }
}


DATA BLOCK: GET THE DATA FROM EXISTING RESOURCES 
WE CAN TAKE DATA FROM ONE WORKSPACE TO ANOTHER WORKSPACE.




provider "aws" {
  region = "us-east-1"
}

resource "aws_instance" "one" {
  ami           = "ami-012967cc5a8c9f891"
  instance_type = "t2.micro"
  tags          = var.tags
}

variable "tags" {
  type = map(string)
  default = {
    Name   = "abc-server"
    env    = "test"
    client = "swiggy"
  }
}

========================================================================================

OPENTOFU: 
its a Forked version of terraform.
it comes into market on 2023.
TERRAFORM is FREE & NOT-OPEN SOURCE & OPENTOFU is FREE & OPEN SOURCE.
TERRAFORM IS MAINTAINED BY HASHICORP BUT OPEN TOFU MAINTED BY COMMUNITY.
TERRAFORM HAS LOT OF PROVIDERS & PLUGINS, BUT OPEN TOFU HAVING LIMTED.
TERRAFORM HAS MORE FEATURES, OPENTOFU HAS LESS FEATURES. 
TERRAFORM HAS HCP, BUT OPENTOFU DONT HAVE ANY CLOUD.




wget https://github.com/opentofu/opentofu/releases/download/v1.6.0-alpha3/tofu_1.6.0-alpha_linux_amd64.zip
unzip tofu_1.6.0-alpha3_linux_amd64.zip 
mv tofu /usr/local/bin/tofu 
tofu version


resource "aws_instance" "one" {
  ami           = "ami-0208b77a23d891325"
  instance_type = "t2.micro"
  tags = {
    Name = "virat-server"
  }
}


tofu init
tofu plan
tofu apply
tofu destroy


====================================================================

NODE EXPORTER -- > PROMETHEUS -- > GRAFANA -- > WE CAN SEE DATA FROM DASHBOARDS

NAME		: PROMETHEUS
TYPE		: FREE & OPEN SOURCE
MAJORLY		: CLOUD NATIVE APPLICATIONS
PURPOSE		: TO MONITOR SERVERS
WHAT IT MONITORS: METRICS (CPU, RAM, DISK ----)
STORE DATA	: TSDB [TIME SERIES DATA BASE]
INTEGRATE	: TO ALEART MANAGER LIKE EMAIL, MOBILE, SLACK --
PORT		: 9090
TO SHOW DATA	: PROMQL -- > PROMETHEUS QUERY LANGUAGE

TOOL		: NODE EXPORTER
TYPE		: ITS A DATA SOURCE
PURPOSE		: TO SEND METRIC FROM WORKER NODE TO PROMETHEUS


TOOL		: GRAFANA
TYPE		: FREE & OPEN SOURCE
PURPOSE		: TO CREATE MONITROING DASHBOARD FOR SERVERS


CONNECTION:
SETUP BOTH PROMETHEUS & GRAFAN FROM BELOW LINK
https://github.com/viratSHAIK007/all-setups.git

PROMETHERUS: 9090
NODE EXPORTER: 9100
GRAFANA: 3000

CONNECTING PROMETHEUS TO GARAFANA:
connect to grafana dashboard -- > Data source -- > add -- > promethus -- > url of prometheus -- > save & test -- > top of page -- > explore data -- > if you want run some queries -- > top -- > import dashboard -- > 1860 -- > laod --- > prometheus -- > import 

CONNECTION TO WORKER NODES
1. CREATE 2 WORKRER NODES
2. GO TO MONITORING SERVER [vim /etc/hosts] ADD IP OF YOUR WORKER NODES

public-ip node1  worker-1
public-ip node2  worker-2

3. INSTALL NODE EXPORTER {WITHOUT NODE EXPORTER WE CANT MONITOR WORKER-NODES}





NOTE: AFTER MODIFIYING PROPERTY OF ANY SERVERCIE WE NEED TO RESTART.
systemctl restart prometeus.sevice

IDS:
1860
10180
14731

===================================================================
GIT: GLOBAL INFORMATION TRACKER
VCS: VERSION CONTROL SYSTEM
SCM: SOURCE CODE MANAGEMENT


VCS: TO STORE EACH VERSION OF CODE SEPERATELY.

V1 : 1 SERVICES : 100 LINES -- > REPO-1
V2 : 2 SERVICES : 200 LINES -- > REPO-2
V3 : 3 SERVICES : 300 LINES -- > REPO-3

WHY WE NEED TO STORE THEM SEPERATELY >
TO DO ROLLBACK: GOING BACK TO PREVIOUS VERSION OF APPLICATION
V3  -- > V2 

V1 : INDEX.HTML : 10
V2 : INDEX.HTML : 20
V3 : INDEX.HTML : 30



WHAT IS GIT?

Git is used to track the files.
It will maintain multiple versions of the same file.
It is platform-independent.
It is free and open-source.
They can handle larger projects efficiently.
It is 3rd generation of vcs.
it is written on c programming
it came on the year 2005

CVCS: CENTRALIZED VERSION CONTROL SYSTEM
STORES CODE ON SINGLE REPO
Ex: SVN

DVCS: DISTRIBUTED VERSION CONTROL SYSTEM
STORES CODE ON MULTIPLE REPO
Ex: GIT


STAGES/ARCHITECTURE OF GIT:

IN GIT WE HAVE TOTAL 3 STAGES:

1. WORKING DIRECTORY	: WHERE WE WRITE THE CODE
2. STAGING AREA		: WHERE WE TRACK THE CODE
3. REPOSITORY		: WHERE WE STORE THE TRACKED THE CODE


PRACTICAL PART:
CREATE AN EC2 INSTANCE AND CONNECT TO BROWSER

yum install git -y : to install git

yum: package manager
install: action
git: package name 
-y : yes 

git --version
git -v


mkdir Paytm
cd Paytm

git init : used to Initialize the empty repository
with out .git commands will not work.


touch index.html	: to create a file
git status		: to show the file status 
git add index.html	: to track the file
git commit -m "commit-1" index.html	: to commit a file
git log			: to show commits history
git log --oneline	: commits on single line
git log --oneline -2	: to show last 2 commits


create -- > track -- > commit
touch  -- > git add -- > git commit

Note: here every dev works on the local laptop
at the end we want all dev codes to create an application.
so here we use GitHub to combine all dev codes together.

Create a GitHub account and create Repo 






git remote add origin https://github.com/revathisiriki78/paytm.git
git push origin movies
username:
password:


Note: in github passwords are not accepted we need to use token 

profile -- > settings -- > developer settings -- > Personal access token -- > classic -- > 
generate new token -- > classic -- > name: paytm -- > select 6 scopes -- > generate 

git show: to show the file that committed for specific commit id.
git show commit_id
=============================================================================
NOTE: WITHOUT INITIAL COMMIT WE CANT SEE MASTER BANCH

USER CONFIGURATION:
git config user.name "VishnuV23"
git config user.email "gstarv23.com"

NOTE:PREVIOUS COMMITS WILL NOTBE AFFECTED.

BRANCHES:
It's an individual line of development for code.
we create different branches in real-time.
each developer will work on their own branch.
At the end we will combine all branches together on GitHub.
Default branch is Master.
Common Branches: Master, Main, Release, Hot-fix -------
 

git branch		: to list the branches
git branch movies	: to create a new branch
git checkout movies	: to switch from one branch to another.
git checkout -b recharge: to create and switch from one branch to another.
git branch -m old new	: to rename a branch
git branch -D movies	: to delete a branch

we cant delete the current branch

PROCESS:

git branch		
git branch movies	
git checkout movies
touch movies{1..5}
git status
git add movies*
git commit -m "dev-1 commits" movies*

git remote add origin https://github.com/revathisiriki78/paytm.git
git push origin master
username:
password:

git push origin movies
username:
password:


git branch dth	
git checkout dth
touch dth{1..5}
git status
git add dth*
git commit -m "dev-2 commits" dth*

git push origin dth
username:
password:


git checkout -b train
touch train{1..5}
git add train*
git commit -m "dev-3 commits" train*

git push origin train
username:
password:


git checkout -b recharge
touch recharge{1..5}
git add recharge*
git commit -m "dev-4 commits" recharge*

git push origin recharge
username:
password:

TO MERGE BRANCHES IN GITHUB WE NEED TO CREATE PULL REQUEST

PULL REQUEST -- > CREATE NEW ULL REQUEST -- > SELECT MOVIES -- > CREATE --> MERGE -- > CONFIRM


GIT MERGE: it will merge files blw two different branches

git checkout master
git merge movies
git merge train


GIT REBASE: used to add files blw two different branches
git checkout master
git rebase dth
git rebase recharge


MERGE VS REBASE:
merge for public repos, rebase for private 
merge stores history, rebase will not store the entire history
merge will show files, rebase will not show files


in GitHub we use Pull Request (PR) to do merging.


GIT REVERT: used to revert(get back) the files we merge.
to undo merge we can use revert.

git revert dth
git revert recharge
=====================

GIT CLONE: it download code from github(Remote) to git(Local).
git clone https://github.com/anitalluri00/paytm.git


GIT FORK: it download code from github account to another.
For git clone and git fork repos must be public.

NOTE: IF WE WANT TO CLONE/FORK FOR PUBLIC REPO WE CAN DO IT DIRECTLY.
FOR PRIVTE REPOSE PREMISSION IS MUST.

PUBLIC REPO: source code will be visible from internet.
PRIVATE REPO: source code will be hidden from internet.

BRANCH PROTECTION RULES:TO RESTRICT THE USER TO NOT DELETE THE BRANCH
SETTINGS -- > BRANCH -- > ADD BRANCH RULE SET -- > Enforcement status: Active 
Target branches --  add target -- > include all brances --> Create

MERGE CONFLICTS:
it will rise when we merge 2 different branches with same files.
How to resolve: Manually 


GIT PULL:
used to get the changed files from github to git.
git pull origin master

GIT FETCH:
used to show the changed files from github to git.
git fetch

CHERRY-PICK: Merging the specific files based on commits.
git cherry-pick commit_id

Process:

mkdir virat
cd virat/
touch java{1..3}
git init
git add java*
git commit -m "java-commits" java*
git branch
git checkout -b branch1
touch python{1..5}
git add python*
git commit -m "python commits" python*
touch php{1..5}
git add php*
git commit -m "php commits" php*
touch donet{1..5}
git add donet*
git commit -m "dontent commits" donet*
git log --oneline
git checkout master
ll
git cherry-pick commit_id_of python
ll
git cherry-pick commit_id_of php


.gitignore: this file will ignore the files of being tracked.
if you write any filename on this .gitingore it wont track that file.
USECASE: cred

Note: . should be mandatory

GIT STATSH: use to hide the files temporarily.
Note: files need to tracked but not committed.

touch file{1..3}
git add file*
git stash       -- > files will be hidden
git stash apply -- > to bring the files back
git stash list  -- > to list stashes
git stash clear -- > to delete all stashes


===========================================================================
MAVEN:

raw chicken -- >  clean  --> marnet -- > ingredients -- > Biryani
raw code    -- > build   -- > test  -- > artifact -- > Deployment

ARTIFACT: its final product of our code.
developers will give raw code that code we are going to convert into artifact.

TYPES:
1. jar	: Java Archive       : Backend code
2. war	: Web Archive	     : Frontend code + Backend code
3. ear	: Enterprise Archive : jar + war 

JAR FILE:

.java -- > compile -- > .class -- > .jar file

.java	: basic raw
.class	: executable file
.jar	: artifact

all the artifacts are going to created by a build too1.

MAVEN:
Maven is the build tool.
its a free and opensource.
build: process of adding the libs & dependencies to code.
its is also called as Project management too1.
it will manage the complete structure of the project.
the main file in the maven tool is POM.XML

POM.XML: its a file which consist of complete project information.
Ex: name, artifact, tools, libs, dep --------

POM: PROJECT OBJECT MODEL
XML: EXTENSIBLE MARKUP LANGUAGE

WHO GIVE POM.XML : DEVELOPERS
dev will give both code and pom.xml in github

maven is written on java by apache software foundation.
supports: JAVA-1.8.0
year: 2004
home path: .m2


PRATCICAL PART:
1. CREATE AN EC2 INSTANCE AND CONNECT
2. yum install git java-1.8.0-openjdk maven tree -y
3. git clone https://github.com/devopsbyvirat/jenkins-java-project.git
4. cd jenkins-java-project



MAVEN LIFECYCLE:
GOALS : a command used to run a task.
Goals refers pom.xml to execute.

NOTE: Without Pom.xml Goals will Not Execute.

PLUGIN: its a small software with makes our work automated.
instead of downloading tools we can download plugins.
this plugins will download automatically when we run goals.


1. mvn compile : used to compile the code (.java [src] -- > .class [target])
2. mvn test    : used to test the code    (.java [test] -- > .class [target])
3. mvn package : used to create artifact 
4. mvn install : used to copy artifact to .m2 (project folder -- > .m2)
5. mvn clean   : to delete the target folder

6. mvn clean package: compile -- > install

mvn clean     : remove old war files
mvn package   : compile -- > test -- > artifact


PROBLEMS WITHOUT MAVEN:
1. we cant create artifacts.
2. We cant create project structure.
3. we cant build and deploy the apps.

ALTERNATIVIES:
MAVEN, ANT, GRADLE 

PROGRAMMING VS BUILD:

JAVA	: MAVEN
PYTHON	: GRADLE
.NET	: VS CODE
C, C#	: MAKE FILE
node.js	: npm

ALTERNETIAVES: 
ANT, GRADLE for java projects.

MAVEN VS ANT:
1. MAVEN IS BUILD & PROJECT MANAGEMNT, ANT IS ONLY BUILD TOOL
2. MAVEN HAS POM.XML, ANT HAS BUILD.XML
3. MAVEN HAS A LIFECYCLE, ANT WILL NOT HAVE LIFECYCLE
4. MAVEN PLUGINS ARE REUSABLE, ANT SCRIPTS ARE NOT RESUEABLE.
5. MAVEN IS DECLARATIVE, ANT IS PROCEDURAL.

SYNOPSIS:
maven is a build tool -- > artifacts -- > java -- > mvn clean package
remove old artifact and compile, test and create artifact to deploy the app
================================================================================

JENINS IS A CI/CD TOOL.
REALITY: JENKINS IS ONLY FOR CI.

CI : CONTINOUS INTEGRATION : CONTINOUS BUILD + CONTINOUS TEST (OLD CODE WITH NEW CODE).

DAY-1: 100 LINES : BUILD + TEST
DAY-2: 200 LINES : BUILD + TEST
DAY-3: 300 LINES : BUILD + TEST

BEFORE CI:
MANUAL PROCESS
TIME WASTE

AFTER CI:
AUTOMATED PROCESS
TIME SAVING

CD: CONTINOUS DELIVERY/DEPLOYMENT

ENV:

PRE-PROD/NON-PROD:
DEV	: developers
QA	: testers
UAT	: clients

LIVE/PROD ENV:
PROD	: users

CONTINOUS DELIVERY: Deploying the application to production in manual.
CONTINOUS DEPLOYMENT: Deploying the application to production in automatic.


PIPELINE: 

CODE -- > COMPILE -- > TEST -- > ARTIFACT -- > DEPLOY

SETP BY STEP EXECUTION OF A PROCESS.
SERIES OF EVENTS INTERLINKED WITH EACHOTHER.

-------------------------------------------
JENKINS: 
ITS A FREE AND OPEN-SOURCE service.
IT IS PLATFORM INDEPENDENT.
JENKINS WRITTEN ON JAVA.
IT CONSIST OF PLUGINS.
WE HAVE COMMUNITY SUPPORT.
IT CAN AUTOMATE ENTIRE SDLC.  
IT IS OWNED BY SUN MICRO SYSTEM AS HUDSON.
HUDSON IS PAID VERSION.
LATER ORACLE BROUGHT HUDSON AND MAKE IT FREE.
LATER HUDSON WAS RENAMED AS JENINS.
INVENTOR: Kohsuke Kawaguchi
PORT NUMBER: 8080
JAVA: JAVA-17
DEFAULT PATH: /var/lib/jenkins

ALTERNATIVES:
BAMBOO, GO CI, CIRCLE CI, TARVIS, SEMAPHORE, BUDDY BUILD MASTER, GITLAB, HARNESS
ARGOCD -----

CLOUD: AWS CODEPIPELINE, AZURE PIPLEINE ---------------------

Hand's On :---->

SETUP: Create an EC2 and Include all traffic/8080 in sg

#STEP-1: INSTALLING GIT JAVA-1.8.0 MAVEN 
yum install git java-1.8.0-openjdk maven -y

#STEP-2: GETTING THE REPO (jenkins.io --> download -- > redhat)
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key

#STEP-3: DOWNLOAD JAVA17 AND JENKINS
yum install java-17-amazon-corretto -y
yum install jenkins -y
update-alternatives --config java

#STEP-4: START JENKINS (when we download service it will on stopped state)
systemctl start jenkins.service
systemctl status jenkins.service

CONNECT:
copy-public-ip:8080 (browser)
cat /var/lib/jenkins/secrets/initialAdminPassword (server)
paste password on browser -- > installing plugins --- > user details -- > start


JOB: it is used to perform task.
to do any work or task in jenkins we need to create a job.

to run commands we need to select execute shell on build steps.

build now: to run job
workspace: place where our job outputs will store

CREATING A JOB:
NEW ITEM -- > NAME: ci-job -- > FREESTYLE -- > OK -- > SCM -- > GIT -- > REPOURL: https://github.com/devopsbyvirat/jenkins-java-project.git -- >Build Steps -- > ADD Build Steps -- > Execute shell -- > mvn clean package -- > save -- > build now

WORKSPACE: where your job output is going to be stored
Default: /var/lib/jenkins/workspace

CUSTOM WORKSPACE: 
GO TO JOB -- > CONFIGURE -- > Advance Options -- > Use custom workspace -- > /root

cd 
mkdir virat
chown jenkins:jenkins /root
chown jenkins:jenkins /root/virat

Now we can see output on root folder after building.


CUSTOM PORT NUMBER:
FOR EVERY SERVICE WE HAVE A CONFIGURATION.
TO CHANGE ANY PROPERTY OF A SERVICE WE NEED TO MODIFY THE CONFIG FILE.

find / -name jenkins.service

vim /usr/lib/systemd/system/jenkins.service
LINE 72: 8080=8090
systemctl daemon-reload
systemctl restart Jenkins

BY DEFAULT JENKINS WILL EXECUTE BUILDS ON SEQUENTIALLY
IF WE WANT TO EXECUTE BUILDS PARALLELY 

GO TO JOB -- > CONFIGURE -- > Execute concurrent builds if necessary -- > SAVE

NOW CLICK ON BUILD NOW 2 TIMES U CAN SEE 2 JOBS RUNNING PARALLELY.

TO INCREASE BUILD EXECUTORS -- > CLICK ON BUILD EXECUTOR STATUS -- > BUILT IN NODE -- > CONFIGURE -- > 2 = 3 -- > SAVE

NOW U CAN MAKE 3 BUILDS AT A TIME

NOT: IN REAL TIME WE PREFER TO EXECUTE ONE BUILD AT A TIME.

=================================================================================

CRON JOB: We can schedule the jobs that need to be run at particular intervals.
here we use cron syntax
cron syntax has * * * * *
each * is separated by space

*	: minutes  
*	: hours 
*	: date
*	: month
*	: day of week (sun=0, mon=1 ----)

59 18 30 11 0

28 19 12 11 3
create a ci job -- > Build Triggers -- > Build periodically -- > * * * * * -- > save

CRONTAB-GENERATOR: https://crontab.guru/

limitation: it will not check the code is changed or not.

http://54.159.60.215:8080/


POLL SCM: 
in pollscm we will set time limit for the jobs.
if dev commit the code it will wait until the time is done.
in given time if we have any changes on code it will generate a build

create a ci job -- > Build Triggers -- > poll scm -- > * * * * * -- > save
commit the changes in GitHub then wait for 1 min.

LIMITATION:
1. in pollscm, we need to wait for the time we set.
2. we will get the last commit only.

WEBHOOK: it will trigger build the moment we change the code.
here we need not to wait for the build.

repo -- > settings -- > webhooks -- > add webhook -- > Payload URL (jenkins url) -- > http://35.180.46.134:8080/github-webhook/  -- > Content type -- > application/json -- > add

create ci job -- > Build Triggers: GitHub hook trigger for GITScm polling -- > save


BUILD SCRIPTS: to make jenkins builds from remote loc using script/
give token 
give url on other browser.


THROTTLE BUILD:

To restrict the builds in a certain time or intervals.
if we dont restrict due to immediate builds jenkins might crashdown.

by default jenkins will not do concurrent builds.
we need to enable this option in configuration.

Execute concurrent builds if necessary -- > tick it

create a ci job -- > configure -- > Throttle builds -- > Number of builds: 3 -- > time period : hours -- > save



CUSTOM THEMES:

dashboard -- > manage Jenkins -- > Appearance -- > You can get more themes atPlugins
Select theme -- > download pugins --  got back to appearance -- > 
Customizable theme -- > CSS
https://github.com/alefnode/jenkins-themes

https://cdn.rawgit.com/afonsof/jenkins-material-theme/gh-pages/dist/material-cyan.css

ADD HEADERS OF RCB AND USE LOGO FROM IMAGES.

2. PASSWORDLESS LOGIN

vim /var/lib/jenkins/config.xml
line-10: true=false
systemctl daemon-reload
systemctl restart jenkins.service

now check the jenkins dashboard it wont ask password

=================================================================================
NOTE: 
WHEN WE STOP SERVICE SERVICES ALSO GOING TO STOP.
TO RESTART ANY SERVICE ACUTOMATIALLY
chkconfig servce_name on

MANAGE JENKINS -- > SYSTEM -- > JENKINS URL -- > PDATE IP & SAVE


MASTER AND SLAVE:
it is used to distribute the builds.
it reduce the load on jenkins server.
communication blw master and slave is ssh.
Here we need to install agent (java-17).
slave can use any platform.
SLAVE IS your instance.
label = way of assigning work for slave.

SETUP:
#STEP-1 : Create a server and install java-17
yum install git java-1.8.0-openjdk maven -y
sudo yum install java-17-amazon-corretto -y

#STEP-2: SETUP THE SLAVE SERVER
Dashboard -- > Manage Jenkins -- > Nodes  -- > New node -- > nodename: abc -- > permanaent agent -- > save 

CONFIGURATION OF SALVE:

Number of executors : 3 #Number of Parallel builds
Remote root directory : /tmp #The place where our output is stored on slave sever.
Labels : one #place the Build in a particular slave
useage: last option
Launch method : last option 
Host: (your privte ip)
Credentials -- > add -- >jenkins -- > Kind : ssh username with privatekey -- > username: ec2-user 
privatekey : pemfile of server -- > save -- > 
Host Key Verification Strategy: last option

DASHBOARD -- > JOB -- > CONFIGURE -- > RESTRTICT WHERE THIS JOB RUN -- > LABEL: SLAVE1 -- > SAVE

BUILD FAILS -- > WHY -- > WE NEED TO INSTALL PACKAGES ON SLAVE NODE
yum install git java-1.8.0-openjdk maven -y


ERROR:
[ERROR] error: Source option 5 is no longer supported. Use 7 or later.
[ERROR] error: Target option 5 is no longer supported. Use 7 or later.

SOL: 

yum remove maven* java* -y
yum install git java-1.8.0-openjdk maven -y


LINKED JOBS:
ONE JOB IS LINKED WITH ANOTHER JOB
IF WE BUILD JOB-1 AUTOMATICALLY JOB-2 IS GOING TO BUILD.

JOB-1 --> CONFIGURE -- > POSTBILD ACTIONS -- > BILD OTHER PROJECTS -- > JOB-2 -- > SAVE
BUILD JOB-1 AND JOB-2 WILL BE TRIGGERED

======================================
PIPELINE: STEP BY EXECUTION OF A PROCESS
SERIES OF EVENTS INTERLINKED WITH EACH OTHER.

code -- > build -- > test -- > artifact -- > deployment

IF PIPELINE FAILS ON STAGE-1 IT WONT GO FOR STAGE-2.

plugin download:
Dashboard
Manage Jenkins
Plugins
available 
pipeline stage view
install
Restart 


why to use ?
to automate the work.
to have clarity about the stage.

TYPES:
1. DECLARATIVE
2. SCRIPTED

pipeline syntax:
to write the pipeline we use DSL.
We use Groovy Script for jenkins Pipeline.
it consists of blocks that include stages.
it includes () & {} braces. 


SHORTCUT: PASSS

P	: PIPELINE
A	: AGENT
S	: STAGES
S	: STAGE
S	: STEPS 


SINGLE STAGE: this pipeline will have only one stage.

EX-1:
pipeline {
    agent any 
    
    stages {
        stage('abc') {
            steps {
               sh 'touch file1'
            }
        }
    }
}

EX-2:
pipeline {
    agent any 
    
    stages {
        stage('Virat') {
            steps {
                sh 'touch file2'
            }
        }
    }
}

MULTI STAGE: this pipeline will have more than one stage.

pipeline {
    agent any
    
    stages {
        stage ('two') {
            steps {
                sh 'lsblk'
            }
        }
        stage ('three') {
            steps {
                sh 'lscpu'
            }
        }
        stage ('four') {
            steps {
                sh 'lsmem'
            }
        }
    }
}


CI PIPELINE:

CODE + BUILD + TEST + ARTIFACT

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyvirat/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
    }
}


PIPELINE AS A CODE: Running more than one command/action inside a single stage.
to reduce the length of the code.
to save the time.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyvirat/jenkins-java-project.git'
                sh 'mvn compile'
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

MULTI STAGE PIPELINE AS A CODE: Running more than one command/action in multiple stages.


pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyvirat/jenkins-java-project.git'
                sh 'mvn compile'
            }
        }
        stage ('two') {
            steps {
                sh 'mvn test'
                sh 'mvn package'
            }
        }
    }
}

PAAC OVER SINGLE SHELL: Running all the shell commands on a single shell.

pipeline {
    agent any
    
    stages {
        stage ('one') {
            steps {
                git 'https://github.com/devopsbyvirat/jenkins-java-project.git'
                sh '''
                mvn compile
                mvn test
                mvn package
                '''
            }
        }
    }
}


INPUT PARAMETERS: BASED ON USER INPUT THE PIPELINE IS GOING TO EXECUTE.
used to avoid the mistakes.
pipeline will wait until we provide the input.

pipeline {
    agent any
    
    stages {
        stage ('checkout') {
            steps {
                git 'https://github.com/devopsbyvirat/jenkins-java-project.git'
            }
        }
        stage ('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage ('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage ('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage ('deploy') {
            input {
                message "is your inputs correct ?"
                ok "yes"
            }
            steps {
                echo "my code is deployed"
            }
        }
    }
}
NOTE: In real time providing manual approval is best practise for Jenkins pipelines.

NOTE: if we have syntax issue none of the stages will execute.

DIFF BLW SCRIPTED VS DECLARATVE

SCRIPTED: 	DECLARATIVE: 
SHORT   	LONG
NO STAGES       IT HAS STAGES
START NODE      START WITH PIPELINE


=======================================================================

NEXUS:
Its an Artifactory storage service.
used to store artifacts on repo. (.JAR, .WAR, .EAR)
Nexus server -- > Repo -- > Artifact
we can use this server to rollback in real time.
it req t2.medium 
nexus uses java-11
PORT: 8081

ALTERTAVIVES: JFROG, S3, -----

SETUP SCIPT:
https://github.com/VishnuV23/all-setups.git

STEPS: signin -- > username: admin & password: /app/sonatype-work/nexus3/admin.password -- > next -- > set password -- > disable anonymous access -- > save

CREATING REPO:
settings symbol -- > repositories -- > new -- > maven2(hosted) -- > name -- > save

NOTE: to integrate any tool with Jenkins we need to download the plugin.

NEXUS INTEGRATION TO PIPELINE:
1. Download the plugin (Nexus Artifact Uploader)
Manage Jenkins -- > plugins -- > Available Plugins -- > Nexus Artifact Uploader -- > install.
2. Configure it to pipeline by using pipeline syntax

NOTE: All the information will be available on pom.xml file.


PIPELINE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git 'https://github.com/devopsbyvishnu/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Artifact upload') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: '968c23dd-b648-4f15-91bf-7d76981a1218', groupId: 'in.virat', nexusUrl: '100.25.197.110:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'netflix', version: '1.2.2'
            }
        }
    }
}


TOMCAT:

WEBSITE: FRONTEND -- > DB IS OPT
WEBAPP: FRONTEND + BACKEND -- > DB IS MANDATORY

WEB APPLICATION SERVER/APPLICATION SERVER/APP SERVER = TOMCAT

ITS A WEB APPLICATION SERVER USED TO DEPLOY JAVA APPLICATIONS.
IT IS WRITTEN ON JAVA LANGUAGE.
AGENT: JAVA


PORT: 8080
WE CAN DEPLOY OUR ARTIFACTS.
ITS FREE AND OPENSOURCE.
Its Platform Independent.
YEAR: 1999 

war : tomcat/webapps
jar : tomcat/lib

ALTERNATIVES: NGINX, IIS, WEBSPHERE, JBOSS, GLASSFISH


SETUP: CREATE A NEW SERVER
INSTALL JAVA: sudo yum install java-17-amazon-corretto -y


STEP-1: DOWNLOAD TOMCAT (dlcdn.apache.org)
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.tar.gz

STEP-2: EXTRACT THE FILES
tar -zxvf apache-tomcat-9.0.97.tar.gz

STEP-3: CONFIGURE USER, PASSWORD & ROLES
vim apache-tomcat-9.0.97/conf/tomcat-users.xml

 56   <role rolename="manager-gui"/>
 57   <role rolename="manager-script"/>
 58   <user username="tomcat" password="virat123" roles="manager-gui, manager-script"/>

STEP-4: DELETE LINE 21 AND 22
vim apache-tomcat-9.0.96/webapps/manager/META-INF/context.xml

STEP-5: STARTING TOMCAT
sh apache-tomcat-9.0.96/bin/startup.sh

CONNECTION:
COPY PUBLIC IP:8080 
manager apps -- > username: tomcat & password: virat123

====================================================================


RBAC:

RBAC: ROLE BASE ACCESS CONTROL.
TO restrict the user PERMISSIONS in jenkins.

yesudas	= fresher
virat	= exp 

STEP-1: USER CREATION
manage jenkins -- > users -- > create users -- > suresh: fresher 

STEP-2: PLUGIN DOWNLOADING
Dashboard
Manage Jenkins
Plugins
Available plugin
Role-based Authorization Strategy  

STEP-3: CONFIGURE THE PLUGIN
Dashboard
Manage Jenkins
Security
Authorization 
Role-based  Strategy  
SAVE

STEP-4: MANAGE AND ASSIGN ROLES TO USERS
manage roles -- > add -- > fresher & exp -- > fresher: overall read & exp: admin -- > save
assign roles -- > add user -- > bahubali: fresher -- > save


RESTRICTING TO SPECIFIC JOB:
MANAGE JENKINS -- > SECURITY --> AUTHORIZATION -- > PROJECT BASED MATRIX AUTHORIZATION STRATEGY -- > 

ADD USER -- > YESUDAS -- > Overall Read


GO TO JOB -- > Enable project-based security
ADD USER -- > YESUDAS 
JOB: BUILD & READ
SAVE


HOW TO ADD PARAMETERS:

PARAMETERS: Used to pass inputs for jobs

CHOICE: to pass single input at a time.
STRING: to pass multiple inputs at a time.
------------
MULTI-LINE STRING: to pass multiple inputs on multiple lines at a time.
FILE: to pass the file as input.
BOOL: to pass input either yes or no.


This project is parameterized
Name: branch 
Choices: 
dev
test
save


===========================================================

CREDS ADDING: 
Dashboard
Manage Jenkins
Credentials
System
Global credentials (unrestricted)
Add credentials 
username & pasword

STEPS FOR PROJECT:

STEP-1: WE CREATE THE CLIENT ACCOUNT IN AWS. [aws orginization]

STEP-2: CREATE THE INFRASTRUCTURE FROM HCP.
        Link: https://github.com/devopsbm/netflixdemoinfra.git

STEP-3: CONFIGURE THE SEVERS [SCRIPTS] [JENKINS, TOMCAT, NEXUS, MONITORING]
        Link: https://github.com/viratSHAIK007/all-setups.git

SETP-4: WRITE PIPELINE FOR CI/CD.
        A. Configure Slave
        B. Download Plugins [pipeline stage view, nexus, Deploy to container, slack] 
        C. Write Pipeline Code

SETP-5: INTEGRATE TO SLACK

STEP-6: MONITOR THE APPLICATION FROM GRAFANA 

TOOLS:
GIT
GITHUB
MAVEN
JENKINS
NEXUS
TOMCAT
HCP
TERRAFORM
SLACK
PROMETHEUS
GRAFANA

PIPELINE CODE:

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git branch: '$branch', url: 'https://github.com/devopsbyvirat/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn package'
            }
        }
        stage('Nexus') {
            steps {
                nexusArtifactUploader artifacts: [[artifactId: 'NETFLIX', classifier: '', file: 'target/NETFLIX-1.2.2.war', type: '.war']], credentialsId: '35cc5458-5e5a-4992-8e90-394d9a3c9ab1', groupId: 'in.virat', nexusUrl: 'ec2-3-80-145-80.compute-1.amazonaws.com:8081', nexusVersion: 'nexus3', protocol: 'http', repository: 'abcd', version: '1.2.2'
            }
        }
        stage('deploy') {
            steps {
                deploy adapters: [
                    tomcat9(
                        credentialsId: '85e5e800-4740-461a-868d-519b51fa90ed',
                        path: '',
                        url: 'http://ec2-54-234-53-68.compute-1.amazonaws.com:8080/'
                    )
                ],
                contextPath: 'netflix',
                war: 'target/*.war'
            }
        }
    }
    post {
        always {
            echo "slack notify"
            slackSend (
                 channel: '#netflix', message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} \n build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"
            )
        }
    }
}

PATH FOR PROMTHEUS: vim /etc/prometheus/prometheus.yml

==========================================================
ANSIBLE:
its a Configuration Management Tool.
Configuration: Hardware and Software.
Management: Pkgs update, installing, remove ----
Ansible is used to manage and work with multiple servers together.
its a free and Opensource.
it is used to automate the entire deployment process on multiple servers.
We install Python3 on Ansible.
we use a key-value format for the playbooks.

PLAYBOOK:
create servers
install packages & software
deploy apps
---------

Jenkins = pipeline = groovy
ansible = playbooks = yaml
YAML: YET ANOTHER MARKUP LANGUAGE

HISTORY:
in 2012 dev called Maichel Dehaan who developed ansible.
After few years RedHat taken the ansible.
it is platform-independent & will work on all linux flavours.


ARCHITECTURE:
PLAYBOOK: its a file which consist of code
INVENTORY: its a file which consist ip of nodes
SSH: used to connect with nodes
Ansible is Agent less.
Means no need to install any software on worker nodes.

SETUP: 
CREATE 3 SERVERS [1=ANSIBLE, 1=DEV, 1=TEST]

EXECUTE THE BELOW COMMANDS ON ALL SERVERS:
sudo -i
hostnamectl set-hostname /dev-1/dev-2/test-1/test-2
sudo -i

passwd root  -- > to login to other servers
vim /etc/ssh/sshd_config (40 & 65 uncomment both lines) 
systemctl restart sshd
systemctl status sshd
hostname -i

THE BELOW STEPS NEED TO BE RUN ON ANSIBLE SERVER:

amazon-linux-extras install ansible2 -y
yum install python3 python-pip python-dlevel -y (opt)

vim /etc/ansible/hosts
# Ex 1: Ungrouped hosts, specify before any group headers.
[dev]
172.31.20.40
172.31.21.25
[test]
172.31.31.77
172.31.22.114

ssh-keygen -- > enter 4 times 
ssh-copy-id root@private ip of dev-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of dev-2 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-1 -- > yes -- > password -- > ssh private ip -- > ctrl d
ssh-copy-id root@private ip of test-2 -- > yes -- > password -- > ssh private ip -- > ctrl d

ansible -m ping all : To check worker node connection with ansible server.

1. ADHOC COMMANDS:
these are simple Linux commands. 
these are used for temp works.
these commands will be over ridden.

ansible all -a "yum install git -y"
ansible all -a "yum install maven -y"
ansible all -a "mvn --version"
ansible all -a "touch file1"
ansible all -a "touch virat.txt"
ansible all -a "ls"
ansible all -a "yum install httpd -y"
ansible all -a "systemctl status httpd"
ansible all -a "systemctl start httpd"
ansible all -a "user add virat"
ansible all -a "cat /etc/passwd"
ansible all -a "yum remove git* maven* httpd* -y"


2. MODULES:
its a key-value pair.
modules are reusable.
we can use different modules for different purposes.
module flag is -m 

ansible all -m yum -a "name=git state=present"
ansible all -m yum -a "name=httpd state=present"	[present=installed]
ansible all -m service -a "name=httpd state=started"	[started=restart]
ansible all -m service -a "name=httpd state=stopped"	[stopped=stop]
ansible all -m yum -a "name=http state=absent"		[absent=uninstall]
ansible all -m user -a "name=vikram state=present"
ansible all -m user -a "name=vikram state=absent"
ansible all -m copy -a "src=virat.txt dest=/tmp"


3. PLAYBOOKS:
playbooks used to execute multiple modules.
we can reuse the playbook multiple times.
in real time we use a playbook to automate our work.
for deployment, pkg installation, Server Creation ----
here we use key-value pairs.
Key-Value can also be called as Dictionary.
ansible-playbook will be written on YAML syntax.
YAML = YET ANOTHER MARKUP LANGUAGE
extension for playbook is .yml or .yaml
playbook start with --- and end with ... (opt)


EX-1:

- hosts: 
  tasks:
    - name: installing git
      yum: name=git state=present

    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: create user
      user: name=jayanth state=present

    - name: copy a file
      copy: src=index.html dest=/root

    

TO EXECUTE: ansible-playbook playbok.yml

Gather facts: it will get information of worker nodes
its by default task performed by ansible.

ok=total number of tasks
changed= no.of tasks successfully executed

EX-2:
 hosts: all
  ignore_errors: true
  tasks:
    - name: installing git
      yum: name=git state=absent
      tags: a

    - name: installing httpd
      yum: name=httpd state=absent
      tags: b

    - name: starting httpd
      service: name=httpd state=started

    - name: create users
      user: name=pushpa state=absent

    - name: copying a file
      copy: src=virat.txt dest=/root


TAGS: by default ansible will execute all tasks sequentially in a playbook.
we can use tags to execute a specific tasks or to skip a specific tasks.


EX-1:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: installing git
      yum: name=git state=present
      tags: a

    - name: installing httpd
      yum: name=httpd state=present
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: create a user
      user: name=kohli state=present
      tags: d

SINGLE TAG: ansible-playbook virat.yml --tags b
MULTI TAGS: ansible-playbook virat.yml --tags b,c

EX-2:

- hosts: all
  ignore_errors: yes
  tasks:
    - name: uninstalling git
      yum: name=git* state=absent
      tags: a

    - name: uninstalling httpd
      yum: name=httpd state=absent
      tags: b

    - name: starting httpd
      service: name=httpd state=started
      tags: c

    - name: delete a user
      user: name=kohli state=absent
      tags: d

    - name: copy a file
      copy: src=index.html dest=/tmp
      tags: e

SKIP A SINGLE TASK: ansible-playbook virat.yml --skip-tags c
SKIP MULTIPLE TASK: ansible-playbook virat.yml --skip-tags a,c



LOOPS: We can use loops to reduce the length of the code for the playbook

- hosts: all
  tasks:
    - name: installing pkg-1
      yum: name={{items}} state=present
      with_items:
        - git
        - java-1.8.0-openjdk
        - maven
        - docker
        - httpd


ansible all -a "git -v"
ansible all -a "java -v"
ansible all -a "maven -v"
ansible all -a "docker -v"
ansible all -a "httpd -v"


- hosts: all
  ignore_errors: true
  tasks:
    - name: creating users
      user: name={{item}} state=present
      with_items:
        - umesh
        - suresh
        - mukesh
        - rajesh
        - ramesh


COME TO ANSIBLE SERVER:
aws configure -- > giving ansible user permissions to server

AWS Access Key ID [None]: xxxxxxxxxxxxx
AWS Secret Access Key [None]: xxxxxxxxxxxx
Default region name [None]: us-east-1
Default output format [None]: table

sudo yum install pip -y
sudo pip install boto

- hosts: localhost
  tasks:
    - name: creating ec2 instance
      ec2:
        region: "us-east-1"
        count: 3
        ami_id: "ami-04ff98ccbfa41c9ad"
        instance_type: "t2.micro"
        instance_tags:
          Name: "abc"

HANDLERS:
when we have two tasks in a single playbook if task 2 is depending upon task 1 so then we can use the concept called handlers .
once task one is executed successfully it will notify task 2 to perform the operation. 
the name of the notify and the name of the task two must be same.


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

sed -i 's/present/absent/g' virat.yml

- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=absent
      notify: starting httpd
  handlers:
    - name: starting httpd
      service: name=httpd state=started

SETUP MODULE: used to print the complete info of worker nodes
ansible all -m setup 

ansible all -m setup  | grep -i family
ansible all -m setup  | grep -i pkg
ansible all -m setup  | grep -i cpus
ansible all -m setup  | grep -i mem


CONDITIONS:
CLUSTER: Group of servers
HOMOGENIUS: all servers have having same OS and flavour.
HETROGENIUS: all servers have different OS and flavour.

used to execute this module when we have different Clusters.

RedHat=yum
Ubuntu=apt

- hosts: all
  tasks:
    - name: installing git on RedHat
      yum: name=git state=present
      when: ansible_os_family == "RedHat"

    - name: installing maven on Debian
      apt: name=maven state=present
      when: ansible_os_family == "Debian"


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present
      when: ansible_nodename == "dev-1"

    - name: installing mysql
      yum: name=mysql state=present
      when: ansible_nodename == "dev-2"

    - name: installing python
      yum: name=python state=present


VALIDATORS:
1. YAMLINT
2. YAML FORMATTER
3. YAML VALIDATOR
4. CHATGPT

SHELL VS COMMAND VS RAW:

- hosts: all
  tasks:
    - name: installing maven
      shell: yum install maven -y

    - name: installing httpd
      command: yum install httpd -y

    - name: installing docker
      raw: yum install docker -y

raw >> command >> shell.

ansible all -a "mvn -v"
ansible all -a "htppd -v"
ansible all -a "docker -v"


L	: LINUX
A	: APACHE
M	: MYSQL
P	: PYTHON


- hosts: all
  tasks:
    - name: installing apache
      yum: name=httpd state=present

    - name: installing mysql
      yum: name=mysql state=present

    - name: installing python
      yum: name=python3 state=present


 ansible all -a "httpd --version"
 ansible all -a "python3 --version"
 ansible all -a "mysql --version"

FRONTEND DEPLOYMENT:


- hosts: all
  tasks:
    - name: installing httpd
      yum: name=httpd state=present

    - name: starting httpd
      service: name=httpd state=started

    - name: installing git
      yum: name=git state=present

    - name: code
      git:
        repo: https://github.com/virat/netflix-clone.git
        dest: "/var/www/html"


VARIABLES:

STATIC VARS: we can define these vars inside the playbook and use for multiple times, once a variable is defined here it will not change untill we change.


- hosts: all
  vars:
    a: maven
    b: httpd
  tasks:
    - name: installing maven
      yum: name={{a}} state=present
    - name: installing httpd
      yum: name={{b}} state=present

TO EXECUTE: ansible-playbook playbbok.yml

DYNAMIC VARS: therse vars will be defined outside the playbook and these will change as per our requirments.

- hosts: all
  vars:
  tasks:
    - name: installing maven
      yum: name={{a}} state=absent
    - name: installing httpd
      yum: name={{b}} state=absent


ansible-playbook virat.yml --extra-vars "a=docker b=httpd"



====================================================


DEBUG: to print the messages from a playbook.

- hosts: all
  tasks:
    - name: printing a msg
      debug:
        msg: hai all welcome to my session

ansible all -m setup

NAME	: ansible_nodename
FAMILY  : ansible_os_family
PKG	: ansible_pkg_mgr
CPU	: ansible_processor_cores
MEM	: ansible_memtotal_mb
FREE	: ansible_memfree_mb


- hosts: all
  tasks:
    - name: print a msg
      debug:
        msg: "my node name is: {{ansible_nodename}}, the os is: {{ansible_os_family}}, the package manager is: {{ansible_pkg_mgr}}, total cpus is: {{ansible_processor_cores}}, the total ram: {{ansible_memtotal_mb}}, free ram is: {{ansible_memfree_mb}}"


JINJA2 TEMPLATE: used to get the customized op, here its a text file which can extract the variables and these values will change as per time.

INLINE MODULE: Used to change specific line in a files on worker node.


FETCH: Used to fetch data in files from remote servers to local.
Note: Create a file in worker nodes

- hosts: all
  tasks:
    - name: fetching data
      fetch: src=/root/Virat.txt dest=/tmp/abc.txt


LOOKUPS: this module used to get data from files, db and key values

- hosts: dev
  vars:
    a: "{{lookup('file', '/root/creds.txt') }}"
  tasks:
    - debug:
        msg: "My user name is {{a}}"



└── roles
    ├── pkgs
    │   └── tasks
    │       └── main.yml
    ├── users
    │   └── tasks
    │       └── main.yml
    └── webserver
        └── tasks
            └── main.yml


ROLES:
roles is a way of organizing playbooks in a structured format.
main purpose of roles is to encapsulate the data.
we can reuse the roles multiple times.
length of the playbook is decreased.
it contains on vars, templates, task -----
in real time we use roles for our daily activities.
yum install tree -y

mkdir playbooks
cd playbooks/

mkdir -p roles/pkgs/tasks
vim roles/pkgs/tasks/main.yml

- name: installing pkgs
  yum: name=git state=present
- name: install maven
  yum: name=maven state=present
- name: installing docker
  yum: name=docker state=present

mkdir -p roles/users/tasks
vim roles/users/tasks/main.yml

- name: create users
  user: name={{item}} state=present
  with_items:
    - uday
    - naveen
    - rohit
    - lokesh
    - saipallavi
    - supriya

mkdir -p roles/webserver/tasks
vim roles/web/tasks/main.yml

- name: installing httpd
  yum: name=httpd state=present

- name: starting httpd
  service: name=httpd state=started

vim master.yml
cat master.yml

- hosts: all
  roles:
    - pkgs
    - users
    - webserver

find . -type f -exec sed -i 's/present/absent/g' {} \;


ANSIBLE GALAXY:

Ansible Galaxy is a  website where users can share roles and to a command-line tool for installing, creating, and managing roles.
Ansible Galaxy gives greater visibility to one of Ansible's most exciting features, such as application installation or reusable roles for server configuration. 
Lots of people share roles in the Ansible Galaxy.
Ansible roles consist of many playbooks, which is a way to group multiple tasks into one container to do the automation in a very effective manner with clean, directory structures.

ANSIBLE VAULT:
it is used to encrypt the files, playbooks ----
Technique: AES256 (USED BY FACEBOOK, AWS)
vault will store our data very safely and securely.
if we want to access any data which is in the vault we need to give a password.
Note: we can restrict the users to access the playbook also.


cat creds.txt
user=virat
passowrd=test123

ansible-vault create creds1.txt		: to create a vault
ansible-vault edit creds1.txt		: to edit a vault
ansible-vault rekey creds1.txt		: to change password for a vault
ansible-vault decrypt creds1.txt	: to decrypt the content	
ansible-vault encrypt creds1.txt	: to encrypt the content	
ansible-vault view creds1.txt		: to show the content without decrypt

PIP: its a pkg manager used to install python libs/modules

Redhat: yum
ubuntu: apt
python: pip

- hosts: all
  tasks:
    - name: install pip
      yum: name=pip state=present

    - name: installing NumPy
      pip: name=NumPy state=present

    - name: installing Pandas
      pip: name=Pandas state=present

ASYNCHRONOUS & POLLING ACTIONS:
for every task in  ansible we can set time limit
if the task is not performed in that time limit ansible will stop playbook execution
this is called as asynchronous and polling.

- hosts: all
  ignore_errors: yes
  tasks:
    - name: sleeping
      command: sleep 30
      async: 20
      poll: 10

    - name: install git
      yum: name=git state=present


==========================================================================
LINK FOR SCRIPTS & PLAYBOOKS : https://github.com/virat/all-setups.git
LINK FOR PROJECT: https://github.com/devopsbyvirat/jenkins-java-project.git

Install Jenkins on ansible server and connect to dashboard

SETP-1: PUSHING THE CODE FROM GIT TO GITHUB : CLEAR
STEP-2: PERFORM CI ON JENKINS : CLEAR
STEP-3: STORE ARTIFACT ON NEXUS : CLEAR
STRP-4: INSTALL TOMCAT ON NODES : CLEAR
SETP-5: COPY WAR FILE TO TOMCAT : CLEAR
STEP-6: INSTALLNODE EXPORTER AND MONITOR THE SERVERS



INTEGRTAING ANSIBE WITH JENKINS:

1. install ansible plugin
	
2. manage jenkins -- > tools -- > ansible -- > name: ansible & Path to ansible executables directory: /usr/bin -- > save (NOTE: /usr/bin is a folder where all Linux commands will store)

3. SAMPLE STEP: ANSIBLE PLAYBOOK
Ansible tool: ansible -- > 
Playbook file path in workspace: /etc/ansible/playbook.yml -- > 
Inventory file path in workspace: /etc/ansible/hosts -- > 
SSH CREDS: give creds of ansible & worker nodes -- > 
Disable the host SSH key check -- > 
generate script

pipeline {
    agent any
    
    stages {
        stage('checkout') {
            steps {
                git branch: '$branch', url: 'https://github.com/virat/jenkins-java-project.git'
            }
        }
        stage('build') {
            steps {
                sh 'mvn compile'
            }
        }
        stage('test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('artifact') {
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Nexus') {
            steps {
                echo "my war is copied"
            }
        }
        stage('deploy') {
            steps {
                ansiblePlaybook credentialsId: 'c4656d6f-5371-42e1-b4f6-7bebd693e15f', disableHostKeyChecking: true, installation: 'ansible', inventory: '/etc/ansible/hosts', limit: '$server', playbook: '/etc/ansible/deploy.yml', vaultTmpPath: ''
            }
        }
    }
}


cat playbook.yml
- hosts: all
  tasks:

    - name: task1
      copy:
        src: /var/lib/jenkins/workspace/pipeline/target/NETFLIX-1.2.2.war
        dest: /root/tomcat/webapps


=============================================================


APPLICATION: Collection of services 

MONOLITHIC: multiple services are deployed on single server with single database.
MICRO SERVICES: multiple services are deployed on multiple servers with multiple database.

BASED ON USERS AND APP COMPLEXITY WE NEED TO SELECT THE ARCHITECTURE.

FACTORS AFFECTING FOR USING MICROSERVICES:
F-1: COST 
F-2: MAINTAINANCE

CONTAINERS:
its free of cost and can create multiple containers.
its same as a server/vm.
it will not have any operating system.
os will be on images.
(SERVER=AMI, CONTAINER=IMAGE)

DOCKER:
Its an free & opensource tool.
it is platform independent.
used to create, run & deploy applications on containers.
it is introduced on 2013 by solomenhykes & sebastian phal.
they used GO language to develop the docker.
here we write files on YAML.
before docker user faced lot of problems, but after docker there is no issues with the application.
Docker will use host resources (cpu, mem, n/w, os).
Docker can run on any OS but it natively supports Linux distributions.

CONTAINERIZATION/DOCKERIZATION:
Process of packing an application with its dependencies.
ex: PUBG

APP= PUBG & DEPENDECY = MAPS

os level of virtualization.

VIRTUALIZATION:
able to create resource with our hardware properties.

ARCHITECTURE & COMPONENTS:
client: it will interact with user
user gives commands and it will be executed by docker client

daemon: manages the docker components(images, containers, volumes)

host: where we install docker (ex: linux, windows, macos)

Registry: manages the images.

ARCHITECTURE OF DOCKER:
yum install docker -y    #client
systemctl start docker	 #client,Engine
systemctl status docker


COMMANDS:
docker pull ubuntu	: pull ubuntu image
docker images		: to see list of images
docker run -it --name cont1 ubuntu : to create a container
-it (interactive) - to go inside a container
cat /etc/os-release	: to see os flavour


apt update -y	: to update 
redhat=yum
ubuntu=apt
without update we cant install any pkg in ubuntu


apt install git -y
apt install apache2 -y
service apache2 start
service apache2 status

docker p q		: to exit container
docker ps -a		: to list all containers
docker attach cont_name	: to go inside container
docker stop cont_name	: to stop container
docker start cont_name	: to start container
docker pause cont_name	: to pause container
docker unpause cont_name: to unpause container
docker inspect cont_name: to get complete info of a container
docker rm cont_name	: to delete a container

STOP: will wait to finish all process running inside container
KILL: wont wait to finish all process running inside container

RESOURCE MANAGEMENT:
By default, docker containers will not have any limits for the resources like cpu ram and memory so we need to restrict resource use for container.

docker stats  --> to check live cpu and memory

docker run -it --name cont7 --cpus="0.1" --memory="300mb" ubuntu
docker update cont7 --cpus="0.7" --memory="300mb"
=================================================================

OS LEVEL OF VIRTUALIZATION:
ability to take backup of complete os and reuse it.

docker pull ubuntu
docker run -it --name cont1 ubuntu
apt update -y
apt install mysql-server apache2 python3 -y
touch file{1...5}
apache2 -v
mysql-server --version
python3 --version
ls

ctrl p q

docker commit cont1 virat:v1
docker run -it --name cont2 virat:v1
apache2 -v
mysql-server --version
python3 --version
ls


DOCKERFILE:
it is an automation way to create image.
here we use components to create image.
in Dockerfile D must be Capiatl.
Components also capital.
To write our instructions we need to use components in Dockerfile
This Dockerfile will be Reuseable.
here we can create image directly without container help.
Name: Dockerfile

docker kill $(docker ps -qa)
docker rm $(docker ps -qa)
docker rmi -f $(docker images -qa)

COMPONENTS:

FROM		: used to base image
RUN		: used to run linux commands (During image creation)
CMD		: used to run linux commands (After container creation)
ENTRYPOINT	: high priority than cmd
COPY		: to copy local files to conatiner
ADD		: to copy internet files to conatiner
WORKDIR		: to open req directory
LABEL		: to add labels for docker images
ENV		: to set env variables (inside container)
ARGS		: to pass env variables (outside containers)
EXPOSE		: to give port number


EX-1:
FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
RUN apt install python3 -y
CMD apt install mysql-server -y


docker build -t virat:v1 .
docker run -it --name cont1 virat:v1 

EX-2:
FROM ubuntu
COPY index.html /tmp
ADD https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.97/bin/apache-tomcat-9.0.97.tar.gz /tmp
WORKDIR /tmp
EXPOSE 8080


docker build -t virat:v2 .
docker run -it --name cont2 virat:v2

EX-3:
FROM ubuntu
COPY index.html /tmp
ADD http://dlcdn.apache.org/tomcat/tomcat-9/v9.0.89/bin/apache-tomcat-9.0.89.tar.gz /tmp
WORKDIR /tmp
LABEL author virat

docker build -t virat:v3 .
docker run -it --name cont3 virat:v3

EX-4:

FROM ubuntu
LABEL author virat
ENV client swiggy
ENV server appserver


docker build -t virat:v4 .
docker run -it --name cont4 virat:v4


INDEX.HTML LINK: https://www.w3schools.com/howto/tryit.asp?
filename=tryhow_css_form_icon


NETFLIX-DEPLOYMENT:

yum install git -y
git clone https://github.com/virat/netflix-clone.git
cd netflix-clone/*

Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY * /var/www/html/
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]


docker build -t netflix:v1 .
docker run -it --name netflix1 -p 80:80 netflix:v1

MULTI-STAGE BUILD:
With multi-stage builds, you use multiple FROM statements in your Dockerfile
if we build image-1 from docker file and use that image-1 to build other image.

Dockerfile  -- > image1

Dockerfile
FROM image1  -- > multi-stage build
-----
-------

ADV:
less time
less work
less complexity

===========================================================================
VOLUMES:
It is used to store data inside container.
volume is a simple directory inside container.
containers uses host resources (cpu, ram, rom).
single volume can be shared to multiple containers.
ex: cont-1 (vol1)  --- > cont2 (vol1) & cont3 (vol1) & cont4 (vol1)
at a time we can share single volume to single container only.
every volume will store under /var/lib/docker/volumes

METHOD-1:
DOCKER FILE:

FROM ubuntu
VOLUME ["/volume1"]

docker build -t virat:v1 .
docker run -it --name cont1 virat:v1
cd volume1/
touch file{1..5}
cat>file1
ctrl p q

docker run -it --name cont2 --volumes-from cont1  ubuntu
cd /volume1
ll
touch file{6..10}
ctrl pq

docker attach cont1
cd volume1
ll

docker run -it --name cont3 --volumes-from cont1 ubuntu

METHOD-2:
FROM CLI:

docker run -it --name cont4 -v volume2 ubuntu
cd volume2/
touch java{1..5}
ctrl p q

docker run -it --name cont5 --volumes-from cont4  ubuntu
cd volume2
ll
touch java{6..10}
ctrl p q
docker attach cont4
ls



METHOD-3: VOLUME MOUNTING

docker volume ls 		: to list volumes
docker volume create name	: to create volume
docker volume inspect volume3	: to get info of volume3
cd /var/lib/docker/volumes/volume3/_data 
touch python{1..5}
docker run -it --name cont5 --mount source=volume3,destination=/volue3 ubuntu
docker volume rm 	: to delete volumes
docker volume prune	: to delete unused volumes

HOST -- > CONTAINER:

cd /root
touch virat{1..5}
docker volume inspect volume4
cp * /var/lib/docker/volumes/volume4/_data
docker exec cont5 ls /volume4


RESOURCE MANAGEMENT:
By default, docker containers will not have any limits for the resources like cpu ram and memory so we need to restrict resource use for container.

By default docker containers will use host resources(cpu, ram, rom)
Resource limits of docker container should not exceed the docker host limits.

docker stats  --> to check live cpu and memory

docker run -it --name cont7 --cpus="0.1" --memory="300mb" ubuntu
docker update cont7 --cpus="0.7" --memory="300mb"

JENKINS SETUP BY DOCKER:
docker run -it --name jenkins -p 8080:8080 jenkins/jenkins:lts


===================

vim Dockerfile

FROM ubuntu
RUN apt update -y
RUN apt install apache2 -y
COPY index.html /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

Index.html: take form w3 schools 

docker build -t movies:v1 .
docker run -itd --name movies -p 81:80 movies:v1

docker build -t train:v1 .
docker run -itd --name train -p 82:80 train:v1

docker build -t dth:v1 .
docker run -itd --name dth -p 83:80 dth:v1

docker build -t recharge:v1 .
docker run -itd --name recharge -p 84:80 recharge:v1

docker ps -a -q		: to list container ids
docker kill $(docker ps -a -q) : to kill all containers 
docker rm $(docker ps -a -q) : to remove all containers 

Note: In the above process all the containers are managed and created one by one in real time we manage all the containers at same time so for that purpose we are going to use the concept called Docker compose.


DOCKER COMPOSE:
It's a tool used to manage multiple containers in single host.
we can create, start, stop, and delete all containers together.
we write container information in a file called a compose file.
compose file is in YAML format.
inside the compose file we can give images, ports, and volumes info of containers.
we need to download this tool and use it.

INSTALLATION:
sudo curl -L "https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
ls /usr/local/bin/
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version


In Linux majorly you are having two type of commands first one is inbuilt commands which come with the operating system by default 
second one is download commands we are going to download with the help of yum, apt or Amazon Linux extras.
some commands we can download on binary files.

NOTE: linux will not give some commands, so to use them we need to download seperately
once a command is downloaded we need to move it to /usr/local/bin
because all the user-executed commands in linux will store in /usr/local/bin
executable permission need to execute the command



vim docker-compose.yml

version: '3.8'
services:
  movies:
    image: movies:v1
    ports:
      - "81:80"
  train:
    image: train:v1
    ports:
      - "82:80"
  dth:
    image: dth:v1
    ports:
      - "83:80"
  recharge:
    image: recharge:v1
    ports:
      - "84:80"

COMMANDS:
docker-compose up -d		: to create and start all containers
docker-compose stop		: to stop all containers
docker-compose start		: to start all containers
docker-compose kill		: to kill all containers
docker-compose rm		: to delete all containers
docker-compose down		: to stop and delete all containers
docker-compose pause		: to pause all containers
docker-compose unpause		: to unpause all containers
docker-compose ps -a		: to list the containers managed by compose file
docker-compose images		: to list the images managed by compose file
docker-compose logs		: to show logs of docker compose
docker-compose top		: to show the process of compose containers
docker-compose restart		: to restart all the compose containers



CHANGING THE DEFULT FILE:

by default the docker-compose wil support the following names
docker-compose.yml, docker-compose.yaml, compose.yml, compose.yaml

mv docker-compose.yml virat.yml
docker-compose up -d	: throws an error

docker-compose -f virat.yml up -d
docker-compose -f virat.yml ps
docker-compose -f virat.yml down


images we create on server.
these images will work on only this server.

git (local) -- > github (internet) = to access by others
image (local) -- > dockerhub (internet) = to access by others

Replace your username 

STEPS:
create dockerhub account
create a repo

docker tag movies:v1 vishnuv23/image23
docker login -- > username and password
docker push vishnuv23/image23



docker tag train:v1 vishnuv23/train
docker push vishnuv23/train


docker tag dth:v1 vishnuv23/dth
docker push vishnuv23/dth

docker tag recharge:v1 vishnuv23/recharge
docker push vishnuv23/recharge

docker rmi -f $(docker images -q)
docker pull vishnuv23/movies:latest

DOCKER SAVE:
docker image save swiggy:v1 > swiggy:v1.tar :covert image to file 
docker image history swiggy:v1
docker rmi swiggy:v1
docker images
docker image load < swiggy\:v1.tar

COMMAND TO ZIP:  gzip dummy:v5.tar abc.zip
DECOMPRESS COMMAND: gzip movies:latest.gz -d

COMPRESSING DOCKER IMAGE SIZE:
1. push to dockerhub
2. use multi stage docker build
3. reduce layers
4. use tar balls
======================================================


High Avaliabilty: more than one server
why: if one server got deleted then other server will gives the app

DOCKER SWARM:
its an orchestration tool for containers. 
used to manage multiple containers on multiple servers.
here we create a cluster (group of servers).
in that cluster, we can create same container on multiple servers.
here we have the manager node and worker node.
manager node will create & distribute the container to worker nodes.
worker node's main purpose is to maintain the container.
without docker engine we cant create the cluster.
Port: 2377
worker node will join on cluster by using a token.
manager node will give the token.

SETUP:
create 3 servers
install docker and start the service
hostnamectl set-hostname manager/worker-1/worker-2
Enable 2377 port 

docker swarm init (manager) -- > copy-paste the token to worker nodes
docker node ls

Note: individual containers are not going to replicate.
if we create a service then only containers will be distributed.

SERVICE: it's a way of exposing and managing multiple containers.
in service we can create copy of conatiners.
that container copies will be distributed to all the nodes.

service -- > containers -- > distributed to nodes

http://3.90.239.96:81/
http://18.234.85.163:81/
http://34.234.79.128:81/

docker service create --name movies --replicas 3 -p 81:80 vishnuv23/movies:latest
docker service ls		: to list services
docker service inspect movies	: to get complete info of service
docker service ps movies	: to list the containers of movies
docker service scale movies=10	: to scale in the containers
docker service scale movies=3	: to scale out the containers
docker service rollback movies	: to go previous state
docker service logs movies	: to see the logs
docker service rm movies	: to delete the services.

when scale in it follows lifo pattern.
LIFO MEANS LAST-IN FIRST-OUT.

Note: if we delete a container it will recreate automatically itself.
it is called as self healing.


CLUSTER ACTIVIES:
docker swarm leave (worker)	: to make node inactive from cluster
To activate the node copy the token.
docker node rm node-id (manager): to delete worker node which is on down state
docker node inspect node_id	: to get comple info of worker node
docker swarm join-token manager	: to generate the token to join

Note: we cant delete the node which is ready state
if we want to join the node to cluster again we need to paste the token on worker node


DOCKER NETWORKING:
Docker networks are used to make communication between the multiple containers that are running on same or different docker hosts. 

We have different types of docker networks.
Bridge Network		: SAME HOST
Overlay network		: DIFFERENT HOST
Host Network
None network

BRIDGE NETWORK: It is a default network that container will communicate with each other within the same host.

OVERLAY NETWORK: Used to communicate containers with each other across the multiple docker hosts.

HOST NETWORK: When you Want your container IP and ec2 instance IP same then you use host network

NONE NETWORK: When you don’t Want The container to get exposed to the world, we use none network. It will not provide any network to our container.


To create a network: docker network create network_name
To see the list: docker network ls
To delete a network: docker network rm network_name
To inspect: docker network inspect network_name
To connect a container to the network: docker network connect network_name container_id/name
docker exec -it cont1  /bin/bash
apt update
apt install iputils-ping -y : command to install ping checks
ping ip-address of cont2
crtl p q

To disconnect from the container: docker network disconnect network_name container_name
To prune: docker network prune

DATABASE SETUP:
docker run -itd --name dbcont -e MYSQL_ROOT_PASSWORD=virat123 mysql:9.0.1
docker exec -it dbcont /bin/bash
mysql -u root -p

=====================================================================

ECS & ECR:

ECS:

Amazon Elastic Container Service (Amazon ECS) is a highly scalable and fast container management service. 
Used to create, run, stop, and manage containers on a cluster. 
ECS doesn’t have any server and compute power (CPU & Ram).
Can be able to do roll back.
With Amazon ECS, your containers are defined in a task definition that you use to run an individual task or task within a service.
You can run your tasks and services on a serverless infrastructure that's managed by AWS Fargate. 
Alternatively, for more control over your infrastructure, you can run your tasks and services on a cluster of Amazon EC2 instances that you manage.


DOCKER-COMPOSE VS ECS:

Docker compose cannot able to do automatic load balancing and Auto scaling.
Instead of using K8S, Swarm, Apahe Mesos and NoMad we can use ECS which is alternative for those


ECS LAUNCH TYPES
WORKING WITH EC2:
Here we are going to manage the infrastructure ie instances.
We need to install ECS agent to communicate.
Need to set Firewall.
We need to make sure all the patches is done for recent updates.
Finally you can manage the containers and configure it as per your requirments.

WORKING WITH FARGET:
It follow serverless Architecture.
We don’t have EC2 instances so need not maintain them.
It will create servers on demand.
We will pay for what we use here.


ECS TASKS:
A task definition is required to run Docker containers in Amazon ECS. Simply it is the blue print for containers and how it is deployed.  It's contain.
The Docker image to use with each container in your task for Application.
How much CPU and memory to use with each task or each container within a task
The infrastructure that your tasks are hosted on
The Docker networking mode to use for the containers in your task
The logging configuration to use for your tasks
Whether the task continues to run if the container finishes or fails
The command that the container runs when it's started
Any data volumes that are used with the containers in the task
The IAM role that your tasks use

ECS SERVICES:
A service definition defines how to run your Amazon ECS service.
It will ensure that certain tasks are running at all times.
It will restart the containers that is crashed or exited.
For example if you have an application we want 2 instances/containers tp run our application all time, we say this to service then it will create 2 instances/containers and start running our application.
If any instance fail it will restart the task.
LOAD BALANCERS:
The main intention is to distribute the traffic here.
If we have app is deployed we can assign the LB and route the traffic to resources.
If we scale our instance the LB is here able to drive the traffic to newly created instance.



HOSTING AN APPLICATION ON ECS:
UPLOADING IMAGE TO ECR:
Amazon Elastic Container Registry (ECR) is a fully managed container registry.
Easy to store, manage, share, and deploy your container images and artifacts anywhere.
It supports private repositories with resource-based permissions using AWS IAM. 
Specified users or EC2 instances can access your container repositories and images. 
You can use your preferred CLI to push, pull, and manage Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts.
ECR -- > CREATE A REPO -- > VISIBILITY: PUBLIC -- > NAME: virat -- > CREATE 
aws configure -- > PLS GENERATE A NEW USER KEYS
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 247004640985.dkr.ecr.us-east-1.amazonaws.com

docker build -t virat .
docker tag virat:latest 247004640985.dkr.ecr.us-east-1.amazonaws.com/virat:latest
docker push 247004640985.dkr.ecr.us-east-1.amazonaws.com/virat:latest
CREATE ECS CLUSTER
ECS -- > APACHE -- > APP LOAD BALANCER -- > CLUSTER: NAME : virat -- > CREATE


SECOND WAY ECS FARGATE:
ECS -- > CREATE CLUSTER -- > NAME: virat -- > CREATE

TASK DEFINATION -- > CREATE A NEW TASK DEFINATION -- > TASK DEFINATION FAMILY: swiggy-dev-task-defination -- > NAME: Hello & URL: nginxdemos/hello -- > NEXT -- > NEXT -- > 

CREATE SERVICE -- > EXITING CLUSTER: virat -- > SERVICE NAME: HELLOWORLD -- > 
REPLICAS : 3 -- > SG : (ALL TRAFFIC) -- > LOAD BALANCER: APPLICATION -- > NEW -- > NAME : -- >
TG NAME: virat-TG -- > GRACE PEROID : 20



STEPS: WORK ON MUMBAI REGION

1. CREATE EC2 AND INSTALL GIT & DOCKER
yum install git docker -y
systemctl start docker
systemctl status docker

2. GET THE CODE AND CREATE A IMAGE
git clone https://github.com/karishma1521success/swiggy-clone.git
cd swiggy-clone

vim Dockefile

FROM ubuntu
RUN apt update && apt install apache2 -y
COPY * /var/www/html
CMD ["/usr/sbin/apachectl", "-D", "FOREGROUND"]

docker build -t swiggy:v1 .

3. CREATE A REPO ON ECR AND PUSH THE IMAGE

ECR -- > CREATE -- > NAME: SWIGGY -- > SCAN IMAGE -- > CREATE REPO

View push commands
NOTE: USE ACCESS KEY AND SECRET KEYS FOR AUTHENTICATION


4. CREATE A CLUSTER BY USING FARGETE.

5. CREATE TASK DEFINATION.	

6. CREATE SERVICE 

===========================================================================
INTRO:

IT is an open-source container orchestration platform.
It is used to automates many of the manual processes like deploying, managing, and scaling containerized applications.
Kubernetes was developed by GOOGLE using GO Language.
MEM -- > GOOGLE -- > CLUSTER -- > MULTIPLE APPS OF GOOGLE -- > BORG -- > 


ARCHITECTURE:

DOCKER : CNCA
K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION


COMPONENTS:
MASTER:

1. API SERVER: communicate with user (takes command execute & give op)
2. ETCD: database of cluster (stores complete info of a cluster ON KEY-VALUE pair)
3. SCHEDULER: select the worker node to shedule pods (depends on hw of node)
4. CONTROLLER: control the k8s objects (n/w, service, Node)

WORKER:

1. KUBELET : its an agent (it will inform all activites to master) It create containers.
2. KUBEPROXY: it deals with nlw (ip, networks, ports)
3. POD: group of conatiners (inside pod we have app)

Note: all components of a cluster will be created as a pod.

API SERVER          : FATHER
ETCD                : MARRIAGE BROKER
SCHEDULER           : BRIDE
CONTROLLER          : MOTHER 

KUBLET              : SIBBLINGS
KUBE PROXY          : RELATIVES 
POD                 : FUNCTION HALL


CLUSTER TYPES:

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)
kind:
k9s:
kubespray:


2. CLOUD-BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKE = GOOGLE KUBERENETS ENGINE



MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
Here Master and worker runs on same server.
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
It is a platform Independent.
Installing Minikube is simple compared to other tools.

NOTE: But we don't implement this in real-time Prod

REQUIREMENTS:

2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

Kubectl is the command line tool for k8s
if we want to execute commands we need to use kubectl.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

NOTE: When you download a command as binary file it need to be on /usr/local/bin 
because all the commands in linux will be on /usr/local/bin 
and need to give executable permission for that binary file to work as a  command.



POD:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE:

kubectl run pod1 --image vishnuv23/image23
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DECRALATIVE: by using file called manifest file

MANDATORY FEILDS: without these feilds we cant create manifest

apiVersion:
kind:
metadata:
spec:


vim pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: vinodvanama/paytmtrain:latest
      name: cont1

execution: 
kubectl create -f virat.yml
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete -f virat.yml

DRAWBACK: once pod is deleted we can't retrieve the pod.
===========================================================
DAY-02:

LABELS:
Labels are key-value pairs that are attached to pods, RC and services. 
They can be added or modified at the during creation or run time.
Rc manages pods based on labels only.
kubectl run pod-1 --image=nginx  --labels="env=dev, app=swiggy"
kubectl get pods --show-labels


SELECTOR:
Selector filter the Pods with same labels.
There are two kinds of selectors: Equality based and Set base


REPLICASET:

It will create multiple replicas of same Pod.
If One Pod deleted it will automatically create new Pod.
All the pods will have same config. (from Template)
We can do Auto Scaling and Load Balancing Through ReplicaSet.
In Background Replication Controller  will be Responsible to create Replicas.
ReplicaSets will use Labels and Selectors to identify Pods.
Replication Controller is Older Version and ReplicaSet is New Version.

CODE:

vim replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest

COMANDS:
TO CREATE              : kubectl create -f abc.yml      
TO LIST                : kubectl get rs
NODE INFO              : kubectl get pod -o wide
COMPLETE INFO          : kubectl describe rs movies
TO EDIT                : kubectl edit rs movies
TO DELETE              : kubectl delete rs movies
TO SCALE               : kubectl scale rs/movies --replicas=10 (LIFO)
TO SHOW LABELS         : kubectl get pods -l app=Paytm

we cant Rollin and rollout, we cant update the application in rs.

DEPLOYMENT:
It has features of RS and some other extra features like updating and rollbacking to a particular version.
The best part of Deployment is we can do it without downtime.
Deployment will create ReplicaSet, ReplicaSet will created Pods.

CODE:

vim deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest



COMMANDS:
TO CREATE              : kubectl create -f deployement.yml      
TO LIST                : kubectl get rs
NODE INFO              : kubectl get pod -o wide
COMPLETE INFO          : kubectl describe rs movies
TO EDIT                : kubectl edit rs movies
TO DELETE              : kubectl delete rs movies
TO SCALE               : kubectl scale rs/movies --replicas=10 (LIFO)
TO SHOW LABELS         : kubectl get pods -l app=Paytm
IMPERATIVE             :kubectl create deploy movies --image=name --replicas=4


NOTE: TO GET INFO OF  DEPLOYMENT TO FILE
kubectl create deployment movies  --image=name  --replicas=4 --dry-run=client -o yaml > movies-deployment.yml


METRIC SERVER:
if we install metric server in k8s cluster it can collects metrics like cpu, ram -- from all the pods and nodes in cluster.
we can use kubectl top po/no to see metrics
previously we can called it as heapster.

Metrics Server offers:

    A single deployment that works on most clusters (see Requirements)
    Fast autoscaling, collecting metrics every 15 seconds.
    Resource efficiency, using 1 milli core of CPU and 2 MB of memory for each node in a cluster.
    Scalable support up to 5,000 node clusters.


You can use Metrics Server for:
CPU/Memory based horizontal autoscaling (Horizontal Autoscaling)
Automatically adjusting/suggesting resources needed by containers (Vertical Autoscaling)


kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
minikube addons enable metrics-server #(only for minikube)


Horizontal: New 
Vertical: Existing



In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or ReplicaSet), with the aim of automatically scaling the workload to match demand.

Example : if you have pod-1 with 50% load and pod2 with 50% load then average will be (50+50/2=50) average value is 50
but if pod-1 is exceeding 60% and pod-2 50% then average will be 55% (then here we need to create a pod-3 becaue its exceeding the average)

Here we need to use metric server whose work is to collect the metrics (cpu & mem info)
metrics server is connected to the HPA and give information to HPA 
Now HPA will analysis metrics for every 30 sec and create a new pod if needed.


COOLING PERIOD: TIME TAKEN BY POD TO REMOVE AFTER STRESS IS GONE


scaling can be done only for scalable objects (ex: RS, Deployment, RC )
HPA is implemented as a K8S API Resources and a controller.
Controller Periodically adjust the number of replicas in RS, RC and Deployment depends on average.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest


kubectl apply -f hpa.yml
kubectl get all
kubectl get deploy 
kubectl autoscale deployment movies --cpu-percent=20 --min=3 --max=10
kubectl get hpa
kubectl desribe hpa movies
kubectl get al1

open second termina and give
kubectl get po --watch

come to first terminal and go inside pod
kubectl exec mydeploy-6bd88977d5-7s6t8 -it -- /bin/bash

apt update -y
apt install stress -y
stress 

check terminal two to see live pods
==================================================================================

KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, ASG-------------


Minikube -- > single node cluster
All the pods on single node 
if that node got deleted then all pods will be gone.

KOPS:
kOps, also known as Kubernetes operations.
it is an free and  open-source tool.
used to create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
AWS (Amazon Web Services) and GCE (Google Cloud Platform) are currently officially supported, with DigitalOcean, Hetzner and OpenStack in beta support, and Azure in alpha.

ADVANTAGES:
•	Automates the provisioning of AWS and GCE Kubernetes clusters
•	Deploys highly available Kubernetes masters
•	Supports rolling cluster updates
•	Autocompletion of commands in the command line
•	Generates Terraform and CloudFormation configurations
•	Manages cluster add-ons.
•	Supports state-sync model for dry-runs and automatic idempotency
•	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.



STEP-1: GIVING PERMISSIONS 

KOps Is a third party tool if it want to create infrastructure on aws 
aws need to give permission for it so we can use IAM user to allocate permission for the kops tool

IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure (run this command on server)

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATING BUCKET 
aws s3api create-bucket --bucket viratsdevopsbatchmay14102024am.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket viratsdevopsbatchmay14102024am.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://viratsdevopsbatchmay14102024am.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name virats.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name virats.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster virats.k8s.local
 * edit your node instance group: kops edit ig --name=virats.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=virats.k8s.local master-us-east-1a

ERROR:
Error: State Store: Required value: Please set the --state flag or exp                  
SOL: export KOPS_STATE_STORE=s3://viratsdevopsbatchmay14102024am.k8s.local


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=virats.k8s.local nodes-us-east-1a
kops update cluster --name virats.k8s.local --yes --admin 
kops rolling-update cluster --yes

ADMIN ACTIVITIES:
kops edit ig --name=virats.k8s.local master-us-east-1a
kops update cluster --name virats.k8s.local --yes
kops rolling-update cluster

NOTE: In real time we use five node cluster two master nodes and three worker nodes.

NOTE: its My humble request for all of you not to delete the cluster manually and do not delete any server use the below command to delete the cluster.

TO DELETE: kops delete cluster --name virats.k8s.local --yes

=============================================================
NAMESPACES:

NAMESPACE: 

It is used to divide the cluster to multiple teams on real time.
Used for isolating groups of resources within cluster.
By Default we work on Default Name space in K8's.
We create NameSpaces when we work for Prod level Workloads.
If we create pod on one namespace it cant be access by other namespaces.
We can set access limits by RBAC and Limits of Cpu, RAM by Quotas.
It is  applicable only for namespaced objects (e.g. Deployments, Services, etc.)
It wont apply for cluster-wide objects (e.g. StorageClass, Nodes, PV).


CLUSTER: HOUSE
NAMESPACES: ROOM
TEAM MATES: FAMILY MEM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
If dev team create a pod on dev ns testing team cant able to access it.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one node to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

NOTE: Every component of Kubernetes cluster is going to create in the form of pod
And all these pods are going to store on kUBE-SYSTEM ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces
kubectl get po --all-namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view : to see current namespace
kubectl run dev1 --image nginx
kubectl run dev2 --image nginx
kubectl run dev3 --image nginx
kubectl create ns test	: to create namespace
kubectl config set-context --current --namespace=test : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl get po -n dev
kubectl delete pod dev1 -n dev
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods

NOTE: BY DEFAULT K8S NAMESPACE WILL PROVIDE ISOLATION BUT NOT RESTRICTION.
TO RESTRICT THE USER TO ACCESS A NAMESPACE IN REAL TIME WE USE RBAC.
WE CREATE USER, WE GIVE ROLES AND ATTACH ROLE.

alias switch="kubectl config set-context --current"
============================================================================================
SERVICE: 
Used to expose Pods to the users.
If Front end Pod need to communicate with backend pod we use service for it.

COMMAND: kubectl apiresources

TYPES:
CLUSTER-IP
NODE PORT
LOAD BALANCER

COMPONENTS OF SERVICES:
Selector: To select pods
Port: Associated to Service
TargetPort: Associated to Pod
nodePort: Associated to Node
Type: Type of the service


TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.
Ideal for internal communication within a cluster.
Suitable for backend services like databases, caches, or internal APIs 
Preferred in Development and Testing Envs.
Not accessible from outside the cluster.



apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: viratshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: ClusterIP
  selector:
    app: movies
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: 

Node Port Range= 30000 - 32767
NodePort expose Pod on a static port on each node.
if Port is not given it will assign automatically.
if target Port is not Given it takes Port value.
NodePort services are typically used for smaller applications with a lower traffic volume.


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: viratshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: NodePort
  selector:
    app: movies
  ports:
    - port: 80
      nodePort: 31111


NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC)
DRAWBACK:
EXPOSING PUBLIC-IP & PORT 
PORT RESTRICTION.

3. LOADBALACER: 
In LoadBalaner we can expose application externally with the help of Cloud Provider LoadBalancer.
it is used when an application needs to handle high traffic loads and requires automatic scaling and load balancing capabilities.
After the LoadBalancer service is created, the cloud provider will created the Load Balancer.
REPLACE  NodePort with LoadBalancer


apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: viratshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80


RESORCE QUOTAS:

Scheduler checks the  CPU, RAM of node before we Place a Pod
If Node CPU, RAM is not matching for Pod, it select another Node in Cluster.
If any node is not having Sufficient CPU, RAM then pod will go Pending state.
Without limits a container in pod can consume all CPU, RAM of Node.
So we need to set limits in Real time to restrict the Containers.
Note: Limits are set in container level, if we have 2 containers in a pod then set values individually.
A container cant use more Cpu’s than Specified limit, But Not Memory.
Pod will be terminated when it use more than limits and we get OOM error.
To set limits and Request we use Quotas and Limit Ranges in Real time.
LimitRange applies to individual containers or pods.
ResourceQuota applies to the entire namespace.

kubectl create ns dev
kubectl config set-context $(kubectl config current-context) --namespace=dev

vim dev-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi

kubectl create -f dev-quota.yml
kubectl get quota


EX-1: Mentioning Limits  = SAFE WAY

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 512Mi

kubectl create -f dep.yml


kubectl get quota
kubectl get po

NOTE: You get only one Pod because Limit is 1 CPU so its allocated to only one 
Pod in node. Change Values and Try again


===================================================================


PV & PVC:
PV its a cluster wide Group of volumes created by Admin.
User Can select volumes from the Group as per Requirements.
If user want to use Volume he need to create Persistent Volume Claim.
Once PVC is Created k8s will Bind PV based on request and Properties.
To Bound Specific PVC to PV use Labels and Selector.
Each PVC is Bound to only One PV, even if we have Additional storage. 
If we create PVC and No PV is available so PVC will be on Pending state.

RESTRICTIONS:
1. Instances must be on same az as the ebs 
2. EBS supports only a sinlge EC2 instance mounting

pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-07e5c6c3fe273239f
    fsType: ext4

pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

dep.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
     app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: virat
        image: centos
        command: ["/bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: my-pv
          mountPath: "/tmp/persistent"
      volumes:
        - name: my-pv
          persistentVolumeClaim:
            claimName: my-pvc

kubectl exec pvdeploy-86c99cf54d-d8rj4 -it -- /bin/bash
cd /tmp/persistent/
ls
vim virat
exit

now delete the pod and new pod will created then in that pod you will see the same content.

RECLAIM POLICY:
Reclaim Policy determines what happens to a  PV once PVC Deleted.
RETAIN: PV will Available, but not Reuseable by any PVC.
RECYCLE: PV will be available But Data will be deleted.
DELETE: PV will be Deleted Automatically.

ACCESS MODES:
RWO: single node mount the volume as read-write at a time.
ROX: multiple nodes mount the volume as read-only simultaneously.
RWX: multiple nodes mount the volume as read-write simultaneously.
RWOP: volume to be mounted as read-write by a single pod.

STORAGE CLASSES:
In PV & PVC storage is Created and increased Manually. 
Storage Classses will automatically Provisions the storage to Pods.
This is Called as Dynamic Provisioning.\

By using SC no need to Maintain PV.

vim sc.yml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer

kubectl create -f sc.yml
kubectl get sc

DAEMONSET:
used to create Only one pod on each workernode.
Its the old version of Deployment.
if we create a new node a pod will be automatically created.
if we delete a old node a pod will be automatically removed.
daemonsets will not be removed at any case in real time.
Kube-Proxy is Deployed as a DaemonSet in all Nodes.
DeamonSet uses NodeAffinity and Default Scheduler to Place Pods.
USECASES: we can create pods for Logging, Monitoring of nodes.
COMMAND: kubectl get ds

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: movies
  labels:
    app: movies
spec:
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
========================================

ENV VARS:
In k8s we can set env vars using evn feild.
env is array means we can set multiple values.
it has key-value fromat.
to set env vars we use configmaps and secrets.


CONFIG MAPS:
it is used to pass  configuration data to pods in key-value fromat.
we pod is created inject configmap, so data is used as env varibles.
First create configmap and later inject to pod.
But the data should be non confidential data ()
But it does not provider security and encryption.
Limit of config map data in only 1 MB (for more data use volumes).
To pass values from cli use literal.

COMMAND:
kubectl create cm virat --from-literal=password=virat123 [FROM CLI]
kubectl create cm virat --from-file=password=virat123 [FROM FILE]
kubectl get cm
kubectl describe cm virat

CODES:

cat config.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: my-configmap
data:
  user: "virat"
  password: "test123"

cat pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: env-configmap
spec:
  containers:
    - name: app
      image: nginx
      envFrom:
        - configMapRef:
            name: my-configmap



SECERETS:
Used to store sensitive information like passwords, keys ---
it wont encrypt data but it will encode them in base64.
when pod is created inject Secret.
Dont push to github becuse they are not encrypted but encode.
it wont encrypt in etcd also.
Data feild indicates number of secrets in secret.
USE BELOW COMMAND TO ENCODE: echo -n "virat123" | base64
SE BELOW COMMAND TO DECODE: echo -n "virat123" | base64 -d

COMMAND:
kubectl create secret generic virat --from-literal=password=virat123
kubectl create secret generic virat --from-file=password=virat123
kubectl get secret virat -o yaml
kubectl describe secret virat

cat secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: my-secrets
data:
  user: "cmFoYW0="
  password: "dGVzdDEyMw=="


cat pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: env-secret
spec:
  containers:
    - name: app
      image: nginx
      envFrom:
        - secretRef:
            name: my-secret


NOTE:
Dont push to github becuse they are not encrypted but encode.
it wont encrypt in etcd also.
Enable Encryption at Rest for Secrets so they stored as encrypted in ETCD
secret is not written to disk storage, Kubelet stores them to tmpfs.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

=================================================================================

HELM:

In K8S Helm is a package manager to install packages
in Redhat: yum & Ubuntu: apt & K8s: helm 

it is used to install applications on clusters.
we can install and deploy applications by using helm
it manages k8s resources packages through charts 
chart is collection of manifest files.
chart is a collection of files organized on a directory structure.
a running instance of a chart with a specific config is called a release.

INSTALLATION OF HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version


INSTALLATION OF METRIC SERVER:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml


INSTALL PROMETHEUS:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts

UPDATE HELM CHART REPOS:
helm repo update
helm repo list

CREATE PROMETHEUS NAMESPACE:
kubectl create namespace prometheus
kubectl get ns

INSTALL PROMETHEUS:
helm install prometheus prometheus-community/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2" --set server.persistentVolume.storageClass="gp2"
kubectl get pods -n prometheus
kubectl get all -n prometheus

CREATE GRAFANA NAMESPACE:
kubectl create namespace grafana

INSTALL GRAFANA:
helm install grafana grafana/grafana --namespace grafana --set persistence.storageClassName="gp2" --set persistence.enabled=true --set adminPassword='viratDevOps' --set  service.type=LoadBalancer
kubectl get pods -n grafana
kubectl get service -n grafana

Copy the EXTERNAL-IP and paste in browser

Go to Grafana Dashboard → Add the Datasource → Select the Prometheus
add the below url in Connection and save and test
http://prometheus-server.prometheus.svc.cluster.local/


Import Grafana dashboard from Grafana Labs
grafana dashboard → new → Import → 6417 → load → select prometheus → import



NOW DEPLOY ANY APPLICATION AND SEE THE RESULT IN DASHBOARD.


ADD 315 PORT TO MONITOR THE FOLLOWING TERMS:
Network I/O pressure.
Cluster CPU usage.
Cluster Memory usage.
Cluster filesystem usage.
Pods CPU usage.

ADD 1860 PORT TO MONITOR NODES INDIVIDUALLY 

11454 -- > for pv and pvcs
747 -- > pod metrics
14623 -- > k8s overview db


MULTI-CONTAINER POD:
It will have more than one container in a pod.
each container have different purpose to work on.
They created and destroyed together and share same n/w and volume.
If any of them fails, the POD restarts.


SIDE CAR:
It creates a helper container to main container.
main container will have the app and helper container Helps main container.

Adapter Design Pattern:
enable communication and coordination between containers.

Ambassador Design Pattern:
used to connect containers with the outside world

Init Container:
it initialize the first work and exits later.

CODE:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: cont1
    image: nginx:1.14.2
  - name: cont2
    image: nginx:1.18


STATIC PODS:
Kubelet Create Pods without API Server is called as Static Pods.
we put Pod manifest in /etc/kubernetes/manifests [on worker node]
Kubelet Read files and Create Pods and Manages it.
If we made any change to Manifest File Kubelet Recreates the Pod.
If we remove file then  pod will be deleted automatically.
USE CASES: used to Deploy control-plane components as a pods in Master.


PRACTICAL:
cd /etc/kubernetes/manifests [Run this on Worker node]
Create a manifest file and run it.
Go to Kops server and Check


===================================================================
INTRO:
Amazon Elastic Kubernetes Service (Amazon EKS) 
it's a service used to run Kubernetes on AWS.
EKS runs control plane across multiple AZs to ensure high availability
EKS Automatically detects and replaces unhealthy control plane nodes.
You can use a range of AWS storage services with Amazon EKS for the storage.


ADVANTAGES:
Fully Managed Service
Easy Cluster Updates
High Availability
Scalability
Security

DISADVANTAGES:

Cost
Regional Availability
Time
Limited Flexibility in Control Plane Management


REAL-TIME USE CASES TO USE EKS:
TO WORK YOUR K8S IN AWS ONLY
ROBUST SCALING
TO REDUCE COMPLEX MANAGEMNETS

SETUP:

#STEP-1: UPDATE

apt update && apt install unzip -y

#STEP-2: EKS & KUBECTL & AWS CLI INSTALLATION

curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws configure

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /usr/local/bin/kubectl


sudo wget https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz
sudo tar -xzvf eksctl_$(uname -s)_amd64.tar.gz -C /usr/local/bin
eksctl version


#STEP-3: CREATE CLUSTER
eksctl create cluster --name=my-cluster --version 1.28 --zones=us-east-1a,us-east-1b,us-east-1c --without-nodegroup

kubectl cluster-info

eksctl utils associate-iam-oidc-provider --region us-east-1 --cluster my-cluster --approve

#STEP-4: CREATE NODEGROUP
eksctl create nodegroup --cluster=my-cluster --region=us-east-1 \
--name=mycluster-ng-1 --node-type=t2.micro \
--nodes=2 --nodes-min=2 --nodes-max=4 --node-volume-size=20 \
--ssh-access  --ssh-public-key=swikp \
--managed --asg-access --external-dns-access --full-ecr-access \
--appmesh-access --alb-ingress-access


eksctl get clusters
eksctl get nodegroup --cluster my-cluster 

#STEP-5: DELETE RESOURCES  
eksctl delete nodegroup --cluster my-cluster --name my-cluster-ng-1 
eksctl delete cluster --name=my-cluster


SCALING:
EKS -- > SELECT CLUSTER -- > COMPUTE -- > EDIT -- > 


CLUSTER UPDATING:
AT A TIME IN K8S WE CAN UPDATE ONE VERSION

1.28 -- > 1.29

TYPES:
1. ROLLING UPDATE: UPDATING THE VERSION FOR ONE BY ONE
2. ALL AT A TIME: ALL THE SERVER WILL BE UPDATE AT A TIME
3. REPLACING: WE WILL REPLACE OLD VERSION  SERVER WITH NEW VERSION SERVERS.


eksctl delete nodegroup --cluster EKS-1 --name my-cluster-ng-1 

=======================================================================
INGRESS IN K8S:
Ingress helps to expose the HTTP and HTTPS routes from outside of the cluster.
Ingress supports 
Path-based  
Host-based routing
Ingress supports Load balancing and SSL termination.
It redirects the incoming requests to the right services based on the Web URL or path in the address.
Ingress provides the encryption feature and helps to balance the load of the applications.

Ingress is used to manage the external traffic to the services within the cluster which provides features like host-based routing, path-based routing, SSL termination, and more. Where a Load balancer is used to manage the traffic but the load balancer does not provide the fine-grained access control like Ingress.

Example:
Suppose you have multiple Kubernetes services running on your cluster and each service serves a different application such as example.com/app1 and example.com/app2. With the help of Ingress, you can achieve this. However, the Load Balancer routes the traffic based on the ports and can't handle the URL-based routing.

To install ingress, firstly we have to install nginx ingress controller:
command: kubectl create -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml

Once we install ingress controller, we have to deploy 2 applications. 
github url: https://github.com/mustafaprojectsindevops/kubernetes/tree/master/ingress
After executing all the files, use kubectl get ing to get ingress. After 30 seconds it will provide one load balancer dns.

access those applications using dns/nginx and dns/httpd. So the traffic will route into both the applications as per the routing

CAST-AI: SHOWS THE COST AND OPTIMZE THE COST FOR K8S CLUSTER
CREATE CLUSTER
CREATE CASTAI ACCOUNT
CONNECT TO CLUSTER -- > SELECT KOPS -- > COPY PASTE COMMANDS IN CLUSTER -- > I RAN SCRIPT
CONNECT CLUSTER
======================================================================================


RBAC:

role : set of permissions for one ns
role binding: adding users to role
these will work on single namespace

cluster role: set of permissions for entire ns
cluster role binding: adding users to cluster role
these will work on all namespaces


when we run kubectl get po k8s api server will authenticate and check authorization
authentication: permission to login
authorization: permission to work on resources

To authenticate API requests, k8s uses the following options:
client certificates,
bearer tokens,
authenticating proxy,
or HTTP basic auth.

Kubernetes doesn’t have an API for creating users.
Though, it can authenticate and authorize users.

We will choose the client certificates as it is the simplest among the four options.

why certs needed on k8s: for authentication purpose.
certs will have users & keys for login.

1. Create a client certificate
We’ll be creating a key and certificate sign request (CSR) needed to create the certificate. Let’s create a directory where to save the certificates. I’ll call it cert:

mkdir dev1 && cd dev1

1. Generate a key using OpenSSL:
openssl genrsa -out dev1.key 2048


2. Generate a Client Sign Request (CSR) :

openssl req -new -key dev1.key -out dev1.csr -subj "/CN=dev1/O=group1"
ls ~/.minikube/

3. Generate the certificate (CRT):

openssl x509 -req -in dev1.csr -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -out dev1.crt -days 500

Now, that we have the .key and the .crt, we can create a user.



Create a user
1. Set a user entry in kubeconfig
kubectl config set-credentials dev1 --client-certificate=dev1.crt --client-key=dev1.key

2.Set a context entry in kubeconfig
kubectl config set-context dev1-context --cluster=minikube --user=dev1
kubectl config view

3.Switching to the created user
kubectl config use-context dev1-context
$ kubectl config current-context # check the current context
dev1-context
But, now, dev1 doesn’t have any access privileges to the cluster. For that we’ll have access denied if we try to create any resource:

$ kubectl create namespace ns-test
kubectl get po
Error from server (Forbidden): namespaces is forbidden: User "dev1" cannot create resource "namespaces" in API group "" at the cluster scope

3. Grant access to the user
To give access to manage k8s resources to dev1, we need to create a Role and a BindingRole.

kubectl config use-context minikube
kubectl create ns dev



3.1. Create a Role

Let’s create a Role object in role.yaml file. The Role will define which resources could be accessed and which operations/verbs could be used.

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
namespace: dev
name: pod-reader
rules:
apiGroups: [" 	"] # "" indicates the core API group
resources: ["pods"]
verbs: ["get", "watch", "list"]
Resources: pod, deployments, namespace, secret, configmap, service, persistentvolume…

Verbs: get, list, watch, create, delete, update, edit, exec.

3.2. Create a BindingRole

We want to match the dev1 to the role created above named : pod-reader. To do that, we need to use RoleBinding.

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: read-pods
namespace: dev
subjects:
kind: User
name: dev1 # Name is case sensitive
apiGroup: rbac.authorization.k8s.io
roleRef:
kind: Role #this must be Role or ClusterRole
name: pod-reader # must match the name of the Role
apiGroup: rbac.authorization.k8s.io
Deploy both the role.yaml and role-binding.yaml to k8s, and don’t forget to use the context of minikube:

kubectl config set-context --current --namespace=dev

$ kubectl config use-context minikube
Switched to context "minikube".
$ kubectl apply -f role.yaml
role.rbac.authorization.k8s.io/pod-reader created
$ kubectl apply -f role-binding.yaml
rolebinding.rbac.authorization.k8s.io/read-pods created
We check that the Role and BindingRole was created successfully:

$ kubectl get roles
NAME AGE
pod-reader 2m
$ kubectl get rolebindings
NAME AGE
read-pods 2m

We used Role to scope the rules to only one namespace, but we can also use ClusterRole to define more than one namespace. RoleBinding is used to bind to Role and ClusterRoleBinding is used to bind to ClusterRole.

4. Testing the allowed operations for user
Switch again to dev1 and try one of the non allowed operations like to create a namespace. This will fail, because dev1 is not allowed to do so.

$ kubectl config use-context dev1-context
$ kubectl create namespace ns-test
$ kubectl get pods

NOTE: TO GIVE PERMISSION FOR ROLE SWITCH TO MINIKUBE CONTEXT
TO CHECK THE GIVEN PERMISSION FOR ROLE SWITCH TO dev1 CONTEXT


CODE:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["pods", "deployments", "namespaces"]
  verbs: ["get", "watch", "list", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: User
  name: dev1
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


K8SGPT:
install kops cluster
curl -LO https://github.com/k8sgpt-ai/k8sgpt/releases/download/v0.3.24/k8sgpt_amd64.rpm
sudo rpm -ivh -i k8sgpt_amd64.rpm
k8sgpt version
k8sgpt auth add
LOGIN TO OPENAI ACCOUNT -- > PROFILE -- > APIKEYS
copy past the key u generate from openai chatgpt
k8sgpt analyze
k8sgpt analyze -o json
k8sgpt analyze -o json | jq .
k8sgpt analyze --explain --filter=Pod --namespace=default

LINK: https://github.com/k8sgpt-ai/k8sgpt

==============================================
=====================================================================================
EFS: ELASTIC FILE SYSTEM
PURPOSE: TO SHARE DATA BLW TWO SERVERS
SIZE: GROW UP TO PETABYTES
TYPE: SERVER LESS
PROTOCOLS: NFSV4.0 & NFSV4.1
INTEGRATIONS: EC2, ECS, EKS, Lambda, Fargate.
TYPES: 1. REGIONAL  2. ONE-ZONE
MODES: 1. GENERAL PURPOSE 2. ELASTIC
PRICING: 5 GB/YEAR FREE
BACKUP: WE CAN GET BACKUPS

EFS -- > CUSTOMIZE -- > NAME: ONE  -- > REGIONAL -- > ELASTIC -- > GIVE SG WITH NFS ENABLE -- > NEXT -- > CREATE.

NOTE: IF NFS IS NOT ENABLE ON SG YOU CANT GET THE DATA.

CREATE 2 SERVERS AND INSTALL & START HTTPD
yum install httpd -y
systemctl start httpd

GO TO EFS -- > ATTACH -- > COPY PASTE COMMANDS ON BOTH SERVERS

sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-05dc344b0bf7f10db.efs.us-east-1.amazonaws.com:/ /var/www/html

change path to /var/www/html

=======================================================

LAMBDA: ITS A SERVERLESS SERVICES.
MEANS WE DONT MANAGE SERVERS (AWS WILL MANAGE)
WE CAN RUN OUR CODE WITHOUT SERVERS.
AWS WILL TAKE CARE OF SERVERS.
IT SUPPORTS MULTIPLE LANGUAGES.
EVEN WE CAN RUN OUR OWN PROGRAMMING LANGUAGE.
FIRST 1M requests / month IS FREE.
PRICING MAY VARY FOR DIFFERNET REGIONS.

WE CAN USE LAMBDA:
MOBILE BACKENDS, 
CHATBOTS, 
IMAGE & DATA PROCESSING, 
GAMING APPS, 
AUTOMATION, 
NOTIFICATIONS

FUNCTION: 

It consists of the code you want to run along with configuration details about how the function should execute.

TYPES:
AUTHOR FROM SCRATCH
USE BLUE PRINT
USE CONTAINER IMAGE

BLUE PRINT -- > SCHEDLE PERODIC CHECK -- > Name -- > Create new role -- > Schedule expression: rate(1 minute) -- > create



CHECKOUT THE CONFIGURATION PART.


EC2 STOP/START:
name
python:3.10
new role for lambda
create

code: https://repost.aws/knowledge-center/start-stop-lambda-eventbridge

deploy
create test event
test

CODE:

import boto3
region = 'us-east-1'
instances = ['i-08104dcdec302fa84']
ec2 = boto3.client('ec2', region_name=region)

def lambda_handler(event, context):
    ec2.start_instances(InstanceIds=instances)
    print('start your instances: ' + str(instances))


import boto3
import os
import json

def lambda_handler(event, context):
    # Initialize S3 client
    s3 = boto3.client('s3')
    
    # Bucket name (can also come from event or environment variables)
    bucket_name = event.get('bucket_name', 'mybucketformlambdacode')
    
    try:
        # Create S3 bucket
        s3.create_bucket(Bucket=bucket_name)
        return {
            'statusCode': 200,
            'body': json.dumps(f"Bucket {bucket_name} created successfully.")
        }
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps(f"Error creating bucket: {str(e)}")
        }

=================================================================

AWS BACKUPS:
USED TO AUTOMATE THE BACKUPS OF AWS SERVICES.
ITS A PAID FEATURE IN AWS.
BY DEFUALT ONLY FEW SERVICES WILL BE ENABLED FOR THE BACKUPS
TO ENABLE ALL SERVICES

SETTINGS -- > ENABLE THE SERVICES.

ON DEMAND BACKUP PLAN
SERVICE: EC2 
RETENTION: 7
CREATE A NEW VAULT
CREATE BACKUP

VAULT -- > EC2-VAULT -- > SELECT INSTANCE -- > ACTION -- > RESTORE
NOTE: SELECT DEFAULT IAM ROLE

AFTER ONE MIN YOU CAN GET THE SERVER BACK.
	
=================================================================
CLOUD FRONT:

Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. 
When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay).
so that content is delivered with the best possible performance.

If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.

If the content is not in that edge location, CloudFront retrieves it from an origin that you've defined—such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.



AWS Free Tier
1 TB of data transfer out
10,000,000 HTTP or HTTPS requests
2,000,000 CloudFront Function invocations
Each month, always free

STEPS:
1. CREATE 2 SERVERS AND ATTACH LOAD BALANCER
2. CLOUD FRONT -- > SELECT ORIGIN -- > ELB: SELECT LB -- > WAF -- > CREATE

RESTRICTIONS:
SECURITY --> -- CloudFront geographic restrictions --> edit -- > block ist -- > india -- > save



